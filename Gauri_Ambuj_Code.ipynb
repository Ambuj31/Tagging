{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/Tag_dataset_tags.csv')\n",
        "\n",
        "# Data Cleaning\n",
        "df.dropna(subset=['Description'], inplace=True)  # Drop rows with missing descriptions\n",
        "df.drop_duplicates(subset=['Description'], keep='first', inplace=True)  # Remove duplicates\n",
        "\n",
        "# Text Preprocessing with Lemmatization and Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "df['lemmatized_description'] = df['Description'].apply(lemmatize_text)\n",
        "\n",
        "# Remove Stopwords from Lemmatized Text\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "df['cleaned_lemmatized_description'] = df['lemmatized_description'].apply(remove_stopwords)\n",
        "\n",
        "# Create Array of Words from Cleaned Lemmatized Description\n",
        "def create_word_array(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['word_array'] = df['cleaned_lemmatized_description'].apply(create_word_array)\n",
        "\n",
        "# Perform POS Tagging on Each Word in Each Array\n",
        "def pos_tag_words(word_array):\n",
        "    return pos_tag(word_array)\n",
        "\n",
        "df['pos_tagged_words'] = df['word_array'].apply(pos_tag_words)\n",
        "\n",
        "# Create Separate Columns for Specific POS Tags\n",
        "def extract_pos_tags(tagged_words, pos_tag):\n",
        "    return ' '.join([word for word, tag in tagged_words if tag == pos_tag])\n",
        "\n",
        "df['Nouns'] = df['pos_tagged_words'].apply(lambda x: extract_pos_tags(x, 'NN') + ' ' + extract_pos_tags(x, 'NNS') + ' ' + extract_pos_tags(x, 'NNP') + ' ' + extract_pos_tags(x, 'NNPS'))\n",
        "df['Verbs'] = df['pos_tagged_words'].apply(lambda x: extract_pos_tags(x, 'VB') + ' ' + extract_pos_tags(x, 'VBD') + ' ' + extract_pos_tags(x, 'VBG') + ' ' + extract_pos_tags(x, 'VBN') + ' ' + extract_pos_tags(x, 'VBP') + ' ' + extract_pos_tags(x, 'VBZ'))\n",
        "df['Adjectives'] = df['pos_tagged_words'].apply(lambda x: extract_pos_tags(x, 'JJ'))\n",
        "# Save the preprocessed data with specific POS tagged words to a new CSV file\n",
        "df.to_csv('Final_pd.csv', index=False)"
      ],
      "metadata": {
        "id": "EkWRPyUeTEtT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "af111315-762d-4450-9b28-60b98325c0f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1839963e3b41>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Tag_dataset_tags.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 )\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# Handle Packages (delegate to a helper function).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_num_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_download_package\u001b[0;34m(self, info, download_dir, force)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mProgressMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                 \u001b[0mnum_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1392\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "RvqNKMpjTZle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "HZ8UGwjqTaSJ",
        "outputId": "740648b5-fcd1-4e72-deab-8340b924a246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "file_path = \"/content/Final_pd.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to extract tags from description\n",
        "def extract_tags(description):\n",
        "    # Tokenize the description\n",
        "    tokens = word_tokenize(description)\n",
        "    # Perform Part-of-Speech tagging\n",
        "    tagged_words = pos_tag(tokens)\n",
        "    # Define your logic to extract tags, for simplicity, let's assume the first noun encountered is the tag\n",
        "    tags = [word for word, tag in tagged_words if tag.startswith('N')]\n",
        "    return tags\n",
        "\n",
        "# Apply the function to each row to generate tags\n",
        "data['Tags'] = data['Description'].apply(extract_tags)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file if needed\n",
        "output_file_path = \"/content/Final_pd_with_tags.csv\"\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BynEJYiz-7Tb",
        "outputId": "cbcef0ab-6435-4535-a8a7-2feb2a9605e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      ID       Name                                        Description  \\\n",
            "0    1.0      Sarah  Sarah is a dedicated student with a passion fo...   \n",
            "1    2.0       Alex  Alex is a tech enthusiast studying Computer Sc...   \n",
            "2    3.0      Emily  Emily is an avid reader and historian, focusin...   \n",
            "3    4.0      James  James is a science enthusiast excelling in Bio...   \n",
            "4    5.0        Mia  Mia is a student of Economics and Political Sc...   \n",
            "5    6.0       Ryan  Ryan is an aspiring engineer fascinated by Rob...   \n",
            "6    7.0     Sophia  Sophia is a psychology enthusiast exploring th...   \n",
            "7    8.0      Lucas  Lucas is an art aficionado studying Art Histor...   \n",
            "8    9.0     Ethan   Ethan is a linguistics and anthropology studen...   \n",
            "9   10.0     Olivia  Olivia is a budding environmental scientist sp...   \n",
            "10  11.0       Lily  Lily is a dedicated student focusing on Chemis...   \n",
            "11  12.0        Max  Max is an aspiring Computer Engineering studen...   \n",
            "12  13.0        Ava  Ava delves into the world of language and cult...   \n",
            "13  14.0       Noah  Noah's academic focus on Economics and Finance...   \n",
            "14  15.0        Zoe  Zoe immerses herself in Political Science and ...   \n",
            "15  16.0        Leo  Leo explores the wonders of the universe throu...   \n",
            "16  17.0     Harper  Harper delves into the realms of Psychology an...   \n",
            "17  18.0      Mason  Mason's pursuits in Mechanical Engineering and...   \n",
            "18  19.0     Amelia  Amelia delves into the world of art through he...   \n",
            "19  20.0     Ethan   Ethan's academic journey in Biology and Geneti...   \n",
            "20  21.0     Sophia  Sophia is a dedicated student focusing on Soci...   \n",
            "21  22.0     Oliver  Oliver is an aspiring tech enthusiast studying...   \n",
            "22  23.0   Isabella  Isabella delves into the world of Environmenta...   \n",
            "23  24.0    William  William is a history enthusiast exploring the ...   \n",
            "24  25.0      Grace  Grace focuses on Public Health and Epidemiolog...   \n",
            "25  26.0   Benjamin  Benjamin's academic pursuits in Mechanical Eng...   \n",
            "26  27.0        Gia  Gia delves into Political Science and Human Ri...   \n",
            "27  28.0      Henry  Henry's academic focus on Finance and Investme...   \n",
            "28  29.0  Charlotte  Charlotte immerses herself in the world of Lit...   \n",
            "29  30.0      Lucas  Lucas is an art enthusiast studying Fine Arts ...   \n",
            "\n",
            "                                                 Tags  \\\n",
            "0   [Sarah, student, passion, Mathematics, Physics...   \n",
            "1   [Alex, enthusiast, Computer, Science, Data, Sc...   \n",
            "2   [reader, Literature, History, joy, Creative, W...   \n",
            "3   [James, science, enthusiast, excelling, Biolog...   \n",
            "4   [Mia, student, Economics, Political, Science, ...   \n",
            "5   [Ryan, engineer, Robotics, Renewable, Energy, ...   \n",
            "6   [Sophia, psychology, enthusiast, intricacies, ...   \n",
            "7   [Lucas, art, aficionado, Art, History, Fine, A...   \n",
            "8   [Ethan, linguistics, anthropology, student, pa...   \n",
            "9   [Olivia, scientist, Environmental, Science, Ge...   \n",
            "10  [student, Chemistry, Environmental, Science, i...   \n",
            "11  [Max, Computer, Engineering, student, interest...   \n",
            "12  [Ava, world, language, culture, studies, Lingu...   \n",
            "13  [Noah, focus, Economics, Finance, interest, in...   \n",
            "14  [Zoe, Political, Science, International, Relat...   \n",
            "15  [Leo, wonders, universe, studies, Physics, Ast...   \n",
            "16  [Harper, realms, Psychology, Behavioral, Econo...   \n",
            "17  [Mason, pursuits, Mechanical, Engineering, Rob...   \n",
            "18  [Amelia, world, art, studies, Art, History, Fi...   \n",
            "19  [Ethan, journey, Biology, Genetics, realm, eng...   \n",
            "20  [Sophia, student, Sociology, Gender, Studies, ...   \n",
            "21  [Oliver, tech, enthusiast, Computer, Science, ...   \n",
            "22  [Isabella, world, Science, Marine, Biology, fo...   \n",
            "23  [William, history, enthusiast, realms, History...   \n",
            "24  [Grace, Public, Health, Epidemiology, passion,...   \n",
            "25  [Benjamin, pursuits, Mechanical, Engineering, ...   \n",
            "26  [Gia, Science, Human, Rights, focus, Internati...   \n",
            "27  [Henry, focus, Finance, Investment, Banking, i...   \n",
            "28  [Charlotte, world, Literature, Creative, Writi...   \n",
            "29  [Lucas, art, enthusiast, Fine, Arts, Sculpture...   \n",
            "\n",
            "                               lemmatized_description  \\\n",
            "0   Sarah is a dedicated student with a passion fo...   \n",
            "1   Alex is a tech enthusiast studying Computer Sc...   \n",
            "2   Emily is an avid reader and historian , focusi...   \n",
            "3   James is a science enthusiast excelling in Bio...   \n",
            "4   Mia is a student of Economics and Political Sc...   \n",
            "5   Ryan is an aspiring engineer fascinated by Rob...   \n",
            "6   Sophia is a psychology enthusiast exploring th...   \n",
            "7   Lucas is an art aficionado studying Art Histor...   \n",
            "8   Ethan is a linguistics and anthropology studen...   \n",
            "9   Olivia is a budding environmental scientist sp...   \n",
            "10  Lily is a dedicated student focusing on Chemis...   \n",
            "11  Max is an aspiring Computer Engineering studen...   \n",
            "12  Ava delf into the world of language and cultur...   \n",
            "13  Noah 's academic focus on Economics and Financ...   \n",
            "14  Zoe immerses herself in Political Science and ...   \n",
            "15  Leo explores the wonder of the universe throug...   \n",
            "16  Harper delf into the realm of Psychology and B...   \n",
            "17  Mason 's pursuit in Mechanical Engineering and...   \n",
            "18  Amelia delf into the world of art through her ...   \n",
            "19  Ethan 's academic journey in Biology and Genet...   \n",
            "20  Sophia is a dedicated student focusing on Soci...   \n",
            "21  Oliver is an aspiring tech enthusiast studying...   \n",
            "22  Isabella delf into the world of Environmental ...   \n",
            "23  William is a history enthusiast exploring the ...   \n",
            "24  Grace focus on Public Health and Epidemiology ...   \n",
            "25  Benjamin 's academic pursuit in Mechanical Eng...   \n",
            "26  Gia delf into Political Science and Human Righ...   \n",
            "27  Henry 's academic focus on Finance and Investm...   \n",
            "28  Charlotte immerses herself in the world of Lit...   \n",
            "29  Lucas is an art enthusiast studying Fine Arts ...   \n",
            "\n",
            "                       cleaned_lemmatized_description  \\\n",
            "0   Sarah dedicated student passion Mathematics Ph...   \n",
            "1   Alex tech enthusiast studying Computer Science...   \n",
            "2   Emily avid reader historian , focusing Literat...   \n",
            "3   James science enthusiast excelling Biology Che...   \n",
            "4   Mia student Economics Political Science , pass...   \n",
            "5   Ryan aspiring engineer fascinated Robotics Ren...   \n",
            "6   Sophia psychology enthusiast exploring intrica...   \n",
            "7   Lucas art aficionado studying Art History Fine...   \n",
            "8   Ethan linguistics anthropology student passion...   \n",
            "9   Olivia budding environmental scientist special...   \n",
            "10  Lily dedicated student focusing Chemistry Envi...   \n",
            "11  Max aspiring Computer Engineering student keen...   \n",
            "12  Ava delf world language culture study Linguist...   \n",
            "13  Noah 's academic focus Economics Finance refle...   \n",
            "14  Zoe immerses Political Science International R...   \n",
            "15  Leo explores wonder universe study Physics Ast...   \n",
            "16  Harper delf realm Psychology Behavioral Econom...   \n",
            "17  Mason 's pursuit Mechanical Engineering Roboti...   \n",
            "18  Amelia delf world art study Art History Fine A...   \n",
            "19  Ethan 's academic journey Biology Genetics lea...   \n",
            "20  Sophia dedicated student focusing Sociology Ge...   \n",
            "21  Oliver aspiring tech enthusiast studying Compu...   \n",
            "22  Isabella delf world Environmental Science Mari...   \n",
            "23  William history enthusiast exploring realm His...   \n",
            "24  Grace focus Public Health Epidemiology , passi...   \n",
            "25  Benjamin 's academic pursuit Mechanical Engine...   \n",
            "26  Gia delf Political Science Human Rights , focu...   \n",
            "27  Henry 's academic focus Finance Investment Ban...   \n",
            "28  Charlotte immerses world Literature Creative W...   \n",
            "29  Lucas art enthusiast studying Fine Arts Sculpt...   \n",
            "\n",
            "                                           word_array  \\\n",
            "0   ['Sarah', 'dedicated', 'student', 'passion', '...   \n",
            "1   ['Alex', 'tech', 'enthusiast', 'studying', 'Co...   \n",
            "2   ['Emily', 'avid', 'reader', 'historian', ',', ...   \n",
            "3   ['James', 'science', 'enthusiast', 'excelling'...   \n",
            "4   ['Mia', 'student', 'Economics', 'Political', '...   \n",
            "5   ['Ryan', 'aspiring', 'engineer', 'fascinated',...   \n",
            "6   ['Sophia', 'psychology', 'enthusiast', 'explor...   \n",
            "7   ['Lucas', 'art', 'aficionado', 'studying', 'Ar...   \n",
            "8   ['Ethan', 'linguistics', 'anthropology', 'stud...   \n",
            "9   ['Olivia', 'budding', 'environmental', 'scient...   \n",
            "10  ['Lily', 'dedicated', 'student', 'focusing', '...   \n",
            "11  ['Max', 'aspiring', 'Computer', 'Engineering',...   \n",
            "12  ['Ava', 'delf', 'world', 'language', 'culture'...   \n",
            "13  ['Noah', \"'s\", 'academic', 'focus', 'Economics...   \n",
            "14  ['Zoe', 'immerses', 'Political', 'Science', 'I...   \n",
            "15  ['Leo', 'explores', 'wonder', 'universe', 'stu...   \n",
            "16  ['Harper', 'delf', 'realm', 'Psychology', 'Beh...   \n",
            "17  ['Mason', \"'s\", 'pursuit', 'Mechanical', 'Engi...   \n",
            "18  ['Amelia', 'delf', 'world', 'art', 'study', 'A...   \n",
            "19  ['Ethan', \"'s\", 'academic', 'journey', 'Biolog...   \n",
            "20  ['Sophia', 'dedicated', 'student', 'focusing',...   \n",
            "21  ['Oliver', 'aspiring', 'tech', 'enthusiast', '...   \n",
            "22  ['Isabella', 'delf', 'world', 'Environmental',...   \n",
            "23  ['William', 'history', 'enthusiast', 'explorin...   \n",
            "24  ['Grace', 'focus', 'Public', 'Health', 'Epidem...   \n",
            "25  ['Benjamin', \"'s\", 'academic', 'pursuit', 'Mec...   \n",
            "26  ['Gia', 'delf', 'Political', 'Science', 'Human...   \n",
            "27  ['Henry', \"'s\", 'academic', 'focus', 'Finance'...   \n",
            "28  ['Charlotte', 'immerses', 'world', 'Literature...   \n",
            "29  ['Lucas', 'art', 'enthusiast', 'studying', 'Fi...   \n",
            "\n",
            "                                     pos_tagged_words  \\\n",
            "0   [('Sarah', 'NNP'), ('dedicated', 'VBD'), ('stu...   \n",
            "1   [('Alex', 'NNP'), ('tech', 'VBD'), ('enthusias...   \n",
            "2   [('Emily', 'RB'), ('avid', 'JJ'), ('reader', '...   \n",
            "3   [('James', 'NNP'), ('science', 'NN'), ('enthus...   \n",
            "4   [('Mia', 'NNP'), ('student', 'NN'), ('Economic...   \n",
            "5   [('Ryan', 'JJ'), ('aspiring', 'VBG'), ('engine...   \n",
            "6   [('Sophia', 'NNP'), ('psychology', 'NN'), ('en...   \n",
            "7   [('Lucas', 'NNP'), ('art', 'NN'), ('aficionado...   \n",
            "8   [('Ethan', 'NNP'), ('linguistics', 'NNS'), ('a...   \n",
            "9   [('Olivia', 'NNP'), ('budding', 'VBG'), ('envi...   \n",
            "10  [('Lily', 'RB'), ('dedicated', 'VBN'), ('stude...   \n",
            "11  [('Max', 'NNP'), ('aspiring', 'VBG'), ('Comput...   \n",
            "12  [('Ava', 'NNP'), ('delf', 'PRP'), ('world', 'N...   \n",
            "13  [('Noah', 'NNP'), (\"'s\", 'POS'), ('academic', ...   \n",
            "14  [('Zoe', 'JJ'), ('immerses', 'VBZ'), ('Politic...   \n",
            "15  [('Leo', 'NNP'), ('explores', 'VBZ'), ('wonder...   \n",
            "16  [('Harper', 'NNP'), ('delf', 'PRP'), ('realm',...   \n",
            "17  [('Mason', 'NNP'), (\"'s\", 'POS'), ('pursuit', ...   \n",
            "18  [('Amelia', 'NNP'), ('delf', 'PRP'), ('world',...   \n",
            "19  [('Ethan', 'NNP'), (\"'s\", 'POS'), ('academic',...   \n",
            "20  [('Sophia', 'NNP'), ('dedicated', 'VBD'), ('st...   \n",
            "21  [('Oliver', 'IN'), ('aspiring', 'VBG'), ('tech...   \n",
            "22  [('Isabella', 'NNP'), ('delf', 'PRP'), ('world...   \n",
            "23  [('William', 'NNP'), ('history', 'NN'), ('enth...   \n",
            "24  [('Grace', 'NNP'), ('focus', 'VBZ'), ('Public'...   \n",
            "25  [('Benjamin', 'NNP'), (\"'s\", 'POS'), ('academi...   \n",
            "26  [('Gia', 'NNP'), ('delf', 'PRP'), ('Political'...   \n",
            "27  [('Henry', 'NNP'), (\"'s\", 'POS'), ('academic',...   \n",
            "28  [('Charlotte', 'NNP'), ('immerses', 'VBZ'), ('...   \n",
            "29  [('Lucas', 'NNP'), ('art', 'NN'), ('enthusiast...   \n",
            "\n",
            "                                                Nouns  \\\n",
            "0   student passion interest lie realm fervor curi...   \n",
            "1   interest revolve technology computing  Alex Co...   \n",
            "2   reader historian inspiration endeavor  Literat...   \n",
            "3   science enthusiast interest research experimen...   \n",
            "4   student passionate understanding interest comm...   \n",
            "5   engineer interest technology drive project fut...   \n",
            "6   psychology enthusiast intricacy behavior study...   \n",
            "7   art aficionado interest expression form meanin...   \n",
            "8   student passionate language culture interest L...   \n",
            "9   scientist interest propel study understand sys...   \n",
            "10  student interest lie apply knowledge address c...   \n",
            "11  student interest passion exploration secure sy...   \n",
            "12  world language culture study fascination aim b...   \n",
            "13  focus interest intricacy market trend precisio...   \n",
            "14  passion diplomacy governance interest Diplomac...   \n",
            "15  study capture beauty object mystery cosmos inq...   \n",
            "16  complexity process interest make choice contex...   \n",
            "17  pursuit fascination automation interest Automa...   \n",
            "18  world art focus Art creativity form art trend ...   \n",
            "19  journey realm engineering biotechnology intere...   \n",
            "20  student interest lie structure advocate equali...   \n",
            "21  tech enthusiast interest passion software solu...   \n",
            "22  world marine ecosystem life  Isabella Environm...   \n",
            "23  history enthusiast interest drive research pas...   \n",
            "24  passion initiative dedication health drive res...   \n",
            "25  pursuit reflect fascination interest aerospace...   \n",
            "26  passion right justice scale fuel commitment fr...   \n",
            "27  focus Banking interest market trend risk world...   \n",
            "28  world passion expression word love art power i...   \n",
            "29  art enthusiast interest inspire endeavor art f...   \n",
            "\n",
            "                                                Verbs  \\\n",
            "0                                dedicated   mystery    \n",
            "1                    tech studying shaping   explores   \n",
            "2                           find  focusing drawing      \n",
            "3                              excelling  drive seek    \n",
            "4                            making informed reflect    \n",
            "5                       fascinated aspiring aiming      \n",
            "6                                        exploring      \n",
            "7                          studying  inspire explores   \n",
            "8                                 anthropology drive    \n",
            "9                      budding specializing  protect    \n",
            "10                               focusing dedicated     \n",
            "11                                 aspiring aiming      \n",
            "12                                   understanding      \n",
            "13                                 reflects navigates   \n",
            "14                                contribute immerses   \n",
            "15                        love  unraveling   explores   \n",
            "16                       exploring  realm understand    \n",
            "17                                           reflect    \n",
            "18                                  exploring  study    \n",
            "19                                           explores   \n",
            "20                      dedicated focusing   explores   \n",
            "21        aspiring studying creating ensuring  drive    \n",
            "22             focus committed protecting studying      \n",
            "23              exploring uncovering understanding      \n",
            "24             promoting preventing improving   focus   \n",
            "25                                         pushing      \n",
            "26    promoting advocating advocating marginalized...   \n",
            "27    Engaged analyzing managing  complexity reflects   \n",
            "28                            storytelling   immerses   \n",
            "29                       studying  challenge explores   \n",
            "\n",
            "                                           Adjectives  \n",
            "0                                            universe  \n",
            "1                                 cutting-edge future  \n",
            "2                    avid joy deep narrative creative  \n",
            "3                                   unravel molecular  \n",
            "4                             global dynamic positive  \n",
            "5   Ryan Autonomous sustainable innovative create ...  \n",
            "6                    human mental well-being societal  \n",
            "7              Contemporary creative diverse artistic  \n",
            "8                                                 NaN  \n",
            "9                   environmental seek natural future  \n",
            "10                                 seek environmental  \n",
            "11                         keen drive ethical digital  \n",
            "12                                 linguistic diverse  \n",
            "13  academic financial Engaged complexity economic...  \n",
            "14  Zoe Political driven global international peac...  \n",
            "15                      universe celestial scientific  \n",
            "16  human behavior decision-making seek individual...  \n",
            "17                              innovative mechanical  \n",
            "18         Contemporary Mixed express artistic modern  \n",
            "19  academic lead genetic cutting-edge advancement...  \n",
            "20                        complexity societal diverse  \n",
            "21                                 innovative digital  \n",
            "22                            intricate dynamic ocean  \n",
            "23                     realm Ancient artifact ancient  \n",
            "24                                         well-being  \n",
            "25                       academic boundary innovative  \n",
            "26                   human global understanding legal  \n",
            "27             academic financial financial strategic  \n",
            "28                          creative reflect literary  \n",
            "29       Contemporary creative innovative traditional  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "file_path = \"/content/Final_pd.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to extract tags from description\n",
        "def extract_tags(description):\n",
        "    # Tokenize the description\n",
        "    tokens = word_tokenize(description)\n",
        "    # Perform Part-of-Speech tagging\n",
        "    tagged_words = pos_tag(tokens)\n",
        "    # Define your logic to extract tags, for simplicity, let's assume the first noun encountered is the tag\n",
        "    tags = [word for word, tag in tagged_words if tag.startswith('N')]\n",
        "    return tags\n",
        "\n",
        "# Create a dictionary to map names to tags\n",
        "name_tag_dict = {}\n",
        "\n",
        "# Iterate over each row to generate tags and map them to names\n",
        "for index, row in data.iterrows():\n",
        "    name = row['Name']\n",
        "    description = row['Description']\n",
        "    tags = extract_tags(description)\n",
        "    name_tag_dict[name] = tags\n",
        "\n",
        "# Display the dictionary\n",
        "print(name_tag_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic9mQupDAPL4",
        "outputId": "35f2ff25-983a-46a3-d6c9-84de227c97d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Sarah': ['Sarah', 'student', 'passion', 'Mathematics', 'Physics', 'interests', 'realms', 'Astrophysics', 'Quantum', 'Mechanics', 'mysteries', 'universe', 'fervor', 'curiosity'], 'Alex': ['Alex', 'enthusiast', 'Computer', 'Science', 'Data', 'Science', 'interests', 'Artificial', 'Intelligence', 'Machine', 'Learning', 'cutting-edge', 'technologies', 'future'], 'Emily': ['reader', 'Literature', 'History', 'joy', 'Creative', 'Writing', 'narratives', 'Ancient', 'Civilizations', 'inspiration', 'past', 'endeavors'], 'James': ['James', 'science', 'enthusiast', 'excelling', 'Biology', 'Chemistry', 'interests', 'Genetics', 'Biochemistry', 'research', 'experiments', 'complexities', 'life', 'level'], 'Mia': ['Mia', 'student', 'Economics', 'Political', 'Science', 'passionate', 'dynamics', 'interests', 'International', 'Relations', 'Public', 'Policy', 'commitment', 'impact', 'society', 'decision-making'], 'Ryan': ['Ryan', 'engineer', 'Robotics', 'Renewable', 'Energy', 'interests', 'Systems', 'technologies', 'projects', 'future'], 'Sophia': ['Sophia', 'student', 'Sociology', 'Gender', 'Studies', 'interests', 'Social', 'Justice', 'Intersectionality', 'complexities', 'structures', 'advocates', 'equality', 'communities'], 'Lucas': ['Lucas', 'art', 'enthusiast', 'Fine', 'Arts', 'Sculpture', 'interests', 'Contemporary', 'Art', 'Installation', 'Art', 'endeavors', 'art', 'forms', 'expressions', 'boundaries'], 'Ethan ': ['Ethan', 'journey', 'Biology', 'Genetics', 'realm', 'engineering', 'biotechnology', 'interests', 'Genetic', 'Engineering', 'Biotechnology', 'advancements', 'manipulation', 'applications', 'healthcare'], 'Olivia': ['Olivia', 'scientist', 'Environmental', 'Science', 'Geology', 'interests', 'Climate', 'Change', 'Research', 'Geological', 'Mapping', 'studies', 'Earth', 'systems', 'generations'], 'Lily': ['student', 'Chemistry', 'Environmental', 'Science', 'interests', 'Chemistry', 'Climate', 'Change', 'Mitigation', 'knowledge', 'challenges', 'sustainability'], 'Max': ['Max', 'Computer', 'Engineering', 'student', 'interest', 'Cybersecurity', 'passion', 'Network', 'Security', 'Ethical', 'Hacking', 'exploration', 'secure', 'systems', 'hacking', 'practices', 'world'], 'Ava': ['Ava', 'world', 'language', 'culture', 'studies', 'Linguistics', 'Translation', 'Studies', 'fascination', 'Multilingualism', 'Cross-Cultural', 'Communication', 'gaps', 'foster', 'communities'], 'Noah': ['Noah', 'focus', 'Economics', 'Finance', 'interest', 'intricacies', 'markets', 'Stock', 'Market', 'Analysis', 'Financial', 'Planning', 'complexities', 'trends', 'decision-making', 'precision'], 'Zoe': ['Zoe', 'Political', 'Science', 'International', 'Relations', 'passion', 'diplomacy', 'governance', 'interests', 'Diplomacy', 'Global', 'Affairs', 'aspirations', 'cooperation', 'efforts'], 'Leo': ['Leo', 'wonders', 'universe', 'studies', 'Physics', 'Astronomy', 'love', 'Astrophotography', 'Cosmology', 'beauty', 'objects', 'mysteries', 'cosmos', 'inquiry'], 'Harper': ['Harper', 'realms', 'Psychology', 'Behavioral', 'Economics', 'complexities', 'behavior', 'processes', 'interests', 'Cognitive', 'Psychology', 'Behavioral', 'Economics', 'individuals', 'choices', 'contexts'], 'Mason': ['Mason', 'pursuits', 'Mechanical', 'Engineering', 'Robotics', 'fascination', 'automation', 'biomechanics', 'interests', 'Automation', 'Robotics', 'exploration', 'technologies', 'efficiency', 'functionality', 'systems'], 'Amelia': ['Amelia', 'world', 'art', 'studies', 'Art', 'History', 'Fine', 'Arts', 'focus', 'Contemporary', 'Art', 'Movements', 'Mixed', 'Media', 'creativity', 'forms', 'art', 'trends', 'techniques'], 'Oliver': ['Oliver', 'tech', 'enthusiast', 'Computer', 'Science', 'Software', 'Engineering', 'interests', 'App', 'Development', 'Cybersecurity', 'passion', 'software', 'solutions', 'security'], 'Isabella': ['Isabella', 'world', 'Science', 'Marine', 'Biology', 'focus', 'Marine', 'Conservation', 'Oceanography', 'ecosystems', 'dynamics', 'life'], 'William': ['William', 'history', 'enthusiast', 'realms', 'History', 'Archaeology', 'interests', 'Ancient', 'Civilizations', 'Historical', 'Preservation', 'research', 'stories', 'artifacts', 'understanding', 'cultures'], 'Grace': ['Grace', 'Public', 'Health', 'Epidemiology', 'passion', 'Disease', 'Prevention', 'Global', 'Health', 'initiatives', 'dedication', 'health', 'drives', 'research', 'diseases', 'systems', 'worldwide'], 'Benjamin': ['Benjamin', 'pursuits', 'Mechanical', 'Engineering', 'Aerospace', 'Engineering', 'fascination', 'Space', 'Exploration', 'Aerodynamics', 'interests', 'boundaries', 'aerospace', 'technology', 'drive', 'exploration', 'engineering', 'solutions', 'space', 'travel'], 'Gia': ['Gia', 'Science', 'Human', 'Rights', 'focus', 'International', 'Law', 'Advocacy', 'passion', 'rights', 'justice', 'scale', 'fuels', 'commitment', 'frameworks', 'communities'], 'Henry': ['Henry', 'focus', 'Finance', 'Investment', 'Banking', 'interest', 'Financial', 'Markets', 'Risk', 'Management', 'market', 'trends', 'risks', 'complexities', 'world', 'acumen'], 'Charlotte': ['Charlotte', 'world', 'Literature', 'Creative', 'Writing', 'passion', 'Poetry', 'Fiction', 'Writing', 'expressions', 'words', 'love', 'arts', 'power', 'imagination']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "file_path = \"/content/Final_pd.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to extract tags from description\n",
        "def extract_tags(description):\n",
        "    # Tokenize the description\n",
        "    tokens = word_tokenize(description)\n",
        "    # Perform Part-of-Speech tagging\n",
        "    tagged_words = pos_tag(tokens)\n",
        "    # Define your logic to extract tags, for simplicity, let's assume the first noun encountered is the tag\n",
        "    tags = [word for word, tag in tagged_words if tag.startswith('N')]\n",
        "    return ' '.join(tags)\n",
        "\n",
        "# Preprocess data\n",
        "data['tags'] = data['Description'].apply(extract_tags)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Description'], data['tags'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with CountVectorizer and Multinomial Naive Bayes\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('clf', MultinomialNB())])\n",
        "\n",
        "# Train the model\n",
        "text_clf.fit(X_train, y_train)\n",
        "\n",
        "# Test the model\n",
        "predicted = text_clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", text_clf.score(X_test, y_test))\n",
        "\n",
        "# Example usage: Predict tags for a given phrase\n",
        "def predict_tags(phrase):\n",
        "    predicted_tags = text_clf.predict([phrase])\n",
        "    return predicted_tags\n",
        "\n",
        "# Example usage\n",
        "phrase = \"Student is very much interested in Mathematics and Physics\"\n",
        "predicted_tags = predict_tags(phrase)\n",
        "print(\"Predicted tags for the phrase:\", predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVowohXDBfVP",
        "outputId": "fbe7b127-159d-4df6-a9fa-327cad6d4b67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Predicted tags for the phrase: ['Sarah student passion Mathematics Physics interests realms Astrophysics Quantum Mechanics mysteries universe fervor curiosity']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Solutions"
      ],
      "metadata": {
        "id": "GH7S7s1Cerub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Assuming 'Description' is a column in your DataFrame\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"/content/Final_pd.csv\")\n",
        "\n",
        "# Initialize NLTK tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_and_tokenize(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "df['Processed'] = df['cleaned_lemmatized_description'].apply(preprocess_and_tokenize)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4gJfJNMB1oB",
        "outputId": "c19de574-4763-420d-e44c-d46a2cbf3a4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score_terms(tokens):\n",
        "    # Count term frequencies\n",
        "    term_freq = Counter(tokens)\n",
        "    # Sort terms by frequency\n",
        "    sorted_terms = dict(sorted(term_freq.items(), key=lambda item: item[1], reverse=True))\n",
        "    return sorted_terms\n",
        "\n",
        "# Score and rank terms in descriptions\n",
        "df['Ranked_Terms'] = df['Processed'].apply(score_terms)\n"
      ],
      "metadata": {
        "id": "GYck6cOwNWis"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top_n_tags(ranked_terms, n=7):\n",
        "    # Select the top N terms\n",
        "    return list(ranked_terms.keys())[:n]\n",
        "\n",
        "df['Top_7_Tags'] = df['Ranked_Terms'].apply(lambda x: select_top_n_tags(x, 7))\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "Fm4DNDvsOKVy",
        "outputId": "212bed9c-1833-4e30-cfac-cd8feee5b8d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    ID   Name                                        Description  \\\n",
              "0  1.0  Sarah  Sarah is a dedicated student with a passion fo...   \n",
              "1  2.0   Alex  Alex is a tech enthusiast studying Computer Sc...   \n",
              "2  3.0  Emily  Emily is an avid reader and historian, focusin...   \n",
              "3  4.0  James  James is a science enthusiast excelling in Bio...   \n",
              "4  5.0    Mia  Mia is a student of Economics and Political Sc...   \n",
              "\n",
              "               Tags                             lemmatized_description  \\\n",
              "0           Physics  Sarah is a dedicated student with a passion fo...   \n",
              "1  Computer Science  Alex is a tech enthusiast studying Computer Sc...   \n",
              "2        Literature  Emily is an avid reader and historian , focusi...   \n",
              "3         Chemistry  James is a science enthusiast excelling in Bio...   \n",
              "4         Economics  Mia is a student of Economics and Political Sc...   \n",
              "\n",
              "                      cleaned_lemmatized_description  \\\n",
              "0  Sarah dedicated student passion Mathematics Ph...   \n",
              "1  Alex tech enthusiast studying Computer Science...   \n",
              "2  Emily avid reader historian , focusing Literat...   \n",
              "3  James science enthusiast excelling Biology Che...   \n",
              "4  Mia student Economics Political Science , pass...   \n",
              "\n",
              "                                          word_array  \\\n",
              "0  ['Sarah', 'dedicated', 'student', 'passion', '...   \n",
              "1  ['Alex', 'tech', 'enthusiast', 'studying', 'Co...   \n",
              "2  ['Emily', 'avid', 'reader', 'historian', ',', ...   \n",
              "3  ['James', 'science', 'enthusiast', 'excelling'...   \n",
              "4  ['Mia', 'student', 'Economics', 'Political', '...   \n",
              "\n",
              "                                    pos_tagged_words  \\\n",
              "0  [('Sarah', 'NNP'), ('dedicated', 'VBD'), ('stu...   \n",
              "1  [('Alex', 'NNP'), ('tech', 'VBD'), ('enthusias...   \n",
              "2  [('Emily', 'RB'), ('avid', 'JJ'), ('reader', '...   \n",
              "3  [('James', 'NNP'), ('science', 'NN'), ('enthus...   \n",
              "4  [('Mia', 'NNP'), ('student', 'NN'), ('Economic...   \n",
              "\n",
              "                                               Nouns  \\\n",
              "0  student passion interest lie realm fervor curi...   \n",
              "1  interest revolve technology computing  Alex Co...   \n",
              "2  reader historian inspiration endeavor  Literat...   \n",
              "3  science enthusiast interest research experimen...   \n",
              "4  student passionate understanding interest comm...   \n",
              "\n",
              "                               Verbs                        Adjectives  \\\n",
              "0               dedicated   mystery                           universe   \n",
              "1   tech studying shaping   explores               cutting-edge future   \n",
              "2          find  focusing drawing     avid joy deep narrative creative   \n",
              "3             excelling  drive seek                  unravel molecular   \n",
              "4           making informed reflect            global dynamic positive   \n",
              "\n",
              "                                           Processed  \\\n",
              "0  [sarah, dedicated, student, passion, mathemati...   \n",
              "1  [alex, tech, enthusiast, studying, computer, s...   \n",
              "2  [emily, avid, reader, historian, focusing, lit...   \n",
              "3  [james, science, enthusiast, excelling, biolog...   \n",
              "4  [mia, student, economics, political, science, ...   \n",
              "\n",
              "                                        Ranked_Terms  \\\n",
              "0  {'sarah': 1, 'dedicated': 1, 'student': 1, 'pa...   \n",
              "1  {'science': 2, 'alex': 1, 'tech': 1, 'enthusia...   \n",
              "2  {'creative': 2, 'emily': 1, 'avid': 1, 'reader...   \n",
              "3  {'james': 1, 'science': 1, 'enthusiast': 1, 'e...   \n",
              "4  {'mia': 1, 'student': 1, 'economics': 1, 'poli...   \n",
              "\n",
              "                                          Top_7_Tags  \n",
              "0  [sarah, dedicated, student, passion, mathemati...  \n",
              "1  [science, alex, tech, enthusiast, studying, co...  \n",
              "2  [creative, emily, avid, reader, historian, foc...  \n",
              "3  [james, science, enthusiast, excelling, biolog...  \n",
              "4  [mia, student, economics, political, science, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-286ad63b-1c17-408a-bfc0-5fc6c60bf6e3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Description</th>\n",
              "      <th>Tags</th>\n",
              "      <th>lemmatized_description</th>\n",
              "      <th>cleaned_lemmatized_description</th>\n",
              "      <th>word_array</th>\n",
              "      <th>pos_tagged_words</th>\n",
              "      <th>Nouns</th>\n",
              "      <th>Verbs</th>\n",
              "      <th>Adjectives</th>\n",
              "      <th>Processed</th>\n",
              "      <th>Ranked_Terms</th>\n",
              "      <th>Top_7_Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Sarah</td>\n",
              "      <td>Sarah is a dedicated student with a passion fo...</td>\n",
              "      <td>Physics</td>\n",
              "      <td>Sarah is a dedicated student with a passion fo...</td>\n",
              "      <td>Sarah dedicated student passion Mathematics Ph...</td>\n",
              "      <td>['Sarah', 'dedicated', 'student', 'passion', '...</td>\n",
              "      <td>[('Sarah', 'NNP'), ('dedicated', 'VBD'), ('stu...</td>\n",
              "      <td>student passion interest lie realm fervor curi...</td>\n",
              "      <td>dedicated   mystery</td>\n",
              "      <td>universe</td>\n",
              "      <td>[sarah, dedicated, student, passion, mathemati...</td>\n",
              "      <td>{'sarah': 1, 'dedicated': 1, 'student': 1, 'pa...</td>\n",
              "      <td>[sarah, dedicated, student, passion, mathemati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Alex</td>\n",
              "      <td>Alex is a tech enthusiast studying Computer Sc...</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Alex is a tech enthusiast studying Computer Sc...</td>\n",
              "      <td>Alex tech enthusiast studying Computer Science...</td>\n",
              "      <td>['Alex', 'tech', 'enthusiast', 'studying', 'Co...</td>\n",
              "      <td>[('Alex', 'NNP'), ('tech', 'VBD'), ('enthusias...</td>\n",
              "      <td>interest revolve technology computing  Alex Co...</td>\n",
              "      <td>tech studying shaping   explores</td>\n",
              "      <td>cutting-edge future</td>\n",
              "      <td>[alex, tech, enthusiast, studying, computer, s...</td>\n",
              "      <td>{'science': 2, 'alex': 1, 'tech': 1, 'enthusia...</td>\n",
              "      <td>[science, alex, tech, enthusiast, studying, co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>Emily</td>\n",
              "      <td>Emily is an avid reader and historian, focusin...</td>\n",
              "      <td>Literature</td>\n",
              "      <td>Emily is an avid reader and historian , focusi...</td>\n",
              "      <td>Emily avid reader historian , focusing Literat...</td>\n",
              "      <td>['Emily', 'avid', 'reader', 'historian', ',', ...</td>\n",
              "      <td>[('Emily', 'RB'), ('avid', 'JJ'), ('reader', '...</td>\n",
              "      <td>reader historian inspiration endeavor  Literat...</td>\n",
              "      <td>find  focusing drawing</td>\n",
              "      <td>avid joy deep narrative creative</td>\n",
              "      <td>[emily, avid, reader, historian, focusing, lit...</td>\n",
              "      <td>{'creative': 2, 'emily': 1, 'avid': 1, 'reader...</td>\n",
              "      <td>[creative, emily, avid, reader, historian, foc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>James</td>\n",
              "      <td>James is a science enthusiast excelling in Bio...</td>\n",
              "      <td>Chemistry</td>\n",
              "      <td>James is a science enthusiast excelling in Bio...</td>\n",
              "      <td>James science enthusiast excelling Biology Che...</td>\n",
              "      <td>['James', 'science', 'enthusiast', 'excelling'...</td>\n",
              "      <td>[('James', 'NNP'), ('science', 'NN'), ('enthus...</td>\n",
              "      <td>science enthusiast interest research experimen...</td>\n",
              "      <td>excelling  drive seek</td>\n",
              "      <td>unravel molecular</td>\n",
              "      <td>[james, science, enthusiast, excelling, biolog...</td>\n",
              "      <td>{'james': 1, 'science': 1, 'enthusiast': 1, 'e...</td>\n",
              "      <td>[james, science, enthusiast, excelling, biolog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Mia</td>\n",
              "      <td>Mia is a student of Economics and Political Sc...</td>\n",
              "      <td>Economics</td>\n",
              "      <td>Mia is a student of Economics and Political Sc...</td>\n",
              "      <td>Mia student Economics Political Science , pass...</td>\n",
              "      <td>['Mia', 'student', 'Economics', 'Political', '...</td>\n",
              "      <td>[('Mia', 'NNP'), ('student', 'NN'), ('Economic...</td>\n",
              "      <td>student passionate understanding interest comm...</td>\n",
              "      <td>making informed reflect</td>\n",
              "      <td>global dynamic positive</td>\n",
              "      <td>[mia, student, economics, political, science, ...</td>\n",
              "      <td>{'mia': 1, 'student': 1, 'economics': 1, 'poli...</td>\n",
              "      <td>[mia, student, economics, political, science, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-286ad63b-1c17-408a-bfc0-5fc6c60bf6e3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-286ad63b-1c17-408a-bfc0-5fc6c60bf6e3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-286ad63b-1c17-408a-bfc0-5fc6c60bf6e3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d2427c3e-83b1-4ade-9d91-3b5853eff5a7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2427c3e-83b1-4ade-9d91-3b5853eff5a7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d2427c3e-83b1-4ade-9d91-3b5853eff5a7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.803408430829505,\n        \"min\": 1.0,\n        \"max\": 30.0,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          28.0,\n          16.0,\n          24.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 27,\n        \"samples\": [\n          \"Ethan \",\n          \"Noah\",\n          \"Olivia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Henry's academic focus on Finance and Investment Banking reflects his interest in Financial Markets and Risk Management. Engaged in analyzing market trends and managing financial risks, he navigates the complexities of the financial world with strategic acumen.\",\n          \"Leo explores the wonders of the universe through his studies in Physics and Astronomy. With a love for Astrophotography and Cosmology, he captures the beauty of celestial objects while unraveling the mysteries of the cosmos through scientific inquiry.\",\n          \"William is a history enthusiast exploring the realms of History and Archaeology. His interests in Ancient Civilizations and Historical Preservation drive his research into the past, uncovering the stories and artifacts that shape our understanding of ancient cultures.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 27,\n        \"samples\": [\n          \"Evolution\",\n          \"Politics\",\n          \"Geology\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Henry 's academic focus on Finance and Investment Banking reflects his interest in Financial Markets and Risk Management . Engaged in analyzing market trend and managing financial risk , he navigates the complexity of the financial world with strategic acumen .\",\n          \"Leo explores the wonder of the universe through his study in Physics and Astronomy . With a love for Astrophotography and Cosmology , he capture the beauty of celestial object while unraveling the mystery of the cosmos through scientific inquiry .\",\n          \"William is a history enthusiast exploring the realm of History and Archaeology . His interest in Ancient Civilizations and Historical Preservation drive his research into the past , uncovering the story and artifact that shape our understanding of ancient culture .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_lemmatized_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Henry 's academic focus Finance Investment Banking reflects interest Financial Markets Risk Management . Engaged analyzing market trend managing financial risk , navigates complexity financial world strategic acumen .\",\n          \"Leo explores wonder universe study Physics Astronomy . love Astrophotography Cosmology , capture beauty celestial object unraveling mystery cosmos scientific inquiry .\",\n          \"William history enthusiast exploring realm History Archaeology . interest Ancient Civilizations Historical Preservation drive research past , uncovering story artifact shape understanding ancient culture .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_array\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"['Henry', \\\"'s\\\", 'academic', 'focus', 'Finance', 'Investment', 'Banking', 'reflects', 'interest', 'Financial', 'Markets', 'Risk', 'Management', '.', 'Engaged', 'analyzing', 'market', 'trend', 'managing', 'financial', 'risk', ',', 'navigates', 'complexity', 'financial', 'world', 'strategic', 'acumen', '.']\",\n          \"['Leo', 'explores', 'wonder', 'universe', 'study', 'Physics', 'Astronomy', '.', 'love', 'Astrophotography', 'Cosmology', ',', 'capture', 'beauty', 'celestial', 'object', 'unraveling', 'mystery', 'cosmos', 'scientific', 'inquiry', '.']\",\n          \"['William', 'history', 'enthusiast', 'exploring', 'realm', 'History', 'Archaeology', '.', 'interest', 'Ancient', 'Civilizations', 'Historical', 'Preservation', 'drive', 'research', 'past', ',', 'uncovering', 'story', 'artifact', 'shape', 'understanding', 'ancient', 'culture', '.']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos_tagged_words\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"[('Henry', 'NNP'), (\\\"'s\\\", 'POS'), ('academic', 'JJ'), ('focus', 'NN'), ('Finance', 'NNP'), ('Investment', 'NNP'), ('Banking', 'NN'), ('reflects', 'VBZ'), ('interest', 'NN'), ('Financial', 'NNP'), ('Markets', 'NNP'), ('Risk', 'NNP'), ('Management', 'NNP'), ('.', '.'), ('Engaged', 'VBD'), ('analyzing', 'VBG'), ('market', 'NN'), ('trend', 'NN'), ('managing', 'VBG'), ('financial', 'JJ'), ('risk', 'NN'), (',', ','), ('navigates', 'NNS'), ('complexity', 'VBP'), ('financial', 'JJ'), ('world', 'NN'), ('strategic', 'JJ'), ('acumen', 'NNS'), ('.', '.')]\",\n          \"[('Leo', 'NNP'), ('explores', 'VBZ'), ('wonder', 'JJR'), ('universe', 'JJ'), ('study', 'NN'), ('Physics', 'NNP'), ('Astronomy', 'NNP'), ('.', '.'), ('love', 'VB'), ('Astrophotography', 'NNP'), ('Cosmology', 'NNP'), (',', ','), ('capture', 'NN'), ('beauty', 'NN'), ('celestial', 'JJ'), ('object', 'NN'), ('unraveling', 'VBG'), ('mystery', 'NN'), ('cosmos', 'NN'), ('scientific', 'JJ'), ('inquiry', 'NN'), ('.', '.')]\",\n          \"[('William', 'NNP'), ('history', 'NN'), ('enthusiast', 'NN'), ('exploring', 'VBG'), ('realm', 'JJ'), ('History', 'NNP'), ('Archaeology', 'NNP'), ('.', '.'), ('interest', 'NN'), ('Ancient', 'JJ'), ('Civilizations', 'NNP'), ('Historical', 'NNP'), ('Preservation', 'NNP'), ('drive', 'NN'), ('research', 'NN'), ('past', 'NN'), (',', ','), ('uncovering', 'VBG'), ('story', 'NN'), ('artifact', 'JJ'), ('shape', 'NN'), ('understanding', 'VBG'), ('ancient', 'JJ'), ('culture', 'NN'), ('.', '.')]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Nouns\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"focus Banking interest market trend risk world navigates acumen Henry Finance Investment Financial Markets Risk Management \",\n          \"study capture beauty object mystery cosmos inquiry  Leo Physics Astronomy Astrophotography Cosmology \",\n          \"history enthusiast interest drive research past story shape culture  William History Archaeology Civilizations Historical Preservation \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Verbs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \" Engaged analyzing managing  complexity reflects\",\n          \"love  unraveling   explores\",\n          \"  exploring uncovering understanding   \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Adjectives\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"creative reflect literary\",\n          \"innovative mechanical\",\n          \"academic financial Engaged complexity economic financial decision-making\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Processed\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ranked_Terms\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Top_7_Tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map names to their top 5 tags\n",
        "name_to_top_tags = pd.Series(df['Top_7_Tags'].values, index=df['Name']).to_dict()\n",
        "\n",
        "print(name_to_top_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2oRQt38OM0J",
        "outputId": "337e5940-d320-45e1-c745-c34c6fd7500e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Sarah': ['sarah', 'dedicated', 'student', 'passion', 'mathematics', 'physic', 'interest'], 'Alex': ['science', 'alex', 'tech', 'enthusiast', 'studying', 'computer', 'data'], 'Emily': ['creative', 'emily', 'avid', 'reader', 'historian', 'focusing', 'literature'], 'James': ['james', 'science', 'enthusiast', 'excelling', 'biology', 'chemistry', 'interest'], 'Mia': ['mia', 'student', 'economics', 'political', 'science', 'passionate', 'understanding'], 'Ryan': ['ryan', 'aspiring', 'engineer', 'fascinated', 'robotics', 'renewable', 'energy'], 'Sophia': ['sophia', 'dedicated', 'student', 'focusing', 'sociology', 'gender', 'study'], 'Lucas': ['art', 'lucas', 'enthusiast', 'studying', 'fine', 'sculpture', 'interest'], 'Ethan ': ['genetic', 'engineering', 'biotechnology', 'ethan', 'academic', 'journey', 'biology'], 'Olivia': ['environmental', 'olivia', 'budding', 'scientist', 'specializing', 'science', 'geology'], 'Lily': ['chemistry', 'environmental', 'lily', 'dedicated', 'student', 'focusing', 'science'], 'Max': ['ethical', 'hacking', 'max', 'aspiring', 'computer', 'engineering', 'student'], 'Ava': ['study', 'ava', 'delf', 'world', 'language', 'culture', 'linguistics'], 'Noah': ['financial', 'market', 'noah', 'academic', 'focus', 'economics', 'finance'], 'Zoe': ['international', 'diplomacy', 'global', 'zoe', 'immerses', 'political', 'science'], 'Leo': ['leo', 'explores', 'wonder', 'universe', 'study', 'physic', 'astronomy'], 'Harper': ['psychology', 'behavioral', 'economics', 'harper', 'delf', 'realm', 'exploring'], 'Mason': ['mechanical', 'robotics', 'automation', 'mason', 'pursuit', 'engineering', 'reflect'], 'Amelia': ['art', 'amelia', 'delf', 'world', 'study', 'history', 'fine'], 'Oliver': ['software', 'oliver', 'aspiring', 'tech', 'enthusiast', 'studying', 'computer'], 'Isabella': ['marine', 'isabella', 'delf', 'world', 'environmental', 'science', 'biology'], 'William': ['history', 'ancient', 'william', 'enthusiast', 'exploring', 'realm', 'archaeology'], 'Grace': ['health', 'disease', 'grace', 'focus', 'public', 'epidemiology', 'passion'], 'Benjamin': ['engineering', 'aerospace', 'space', 'exploration', 'benjamin', 'academic', 'pursuit'], 'Gia': ['human', 'right', 'advocating', 'gia', 'delf', 'political', 'science'], 'Henry': ['financial', 'market', 'risk', 'henry', 'academic', 'focus', 'finance'], 'Charlotte': ['creative', 'writing', 'charlotte', 'immerses', 'world', 'literature', 'passion']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# This defaultdict will store counts and names for each tag\n",
        "tag_to_overall_count_and_names = defaultdict(lambda: {\"count\": 0, \"names\": defaultdict(int)})\n",
        "\n",
        "for name, tags in name_to_top_tags.items():\n",
        "    for tag in tags:\n",
        "        # Increase the overall count for the tag\n",
        "        tag_to_overall_count_and_names[tag][\"count\"] += 1\n",
        "        # Increase the count for this tag under this specific name\n",
        "        tag_to_overall_count_and_names[tag][\"names\"][name] += 1\n",
        "\n",
        "# Now, printing results as per your format:\n",
        "for tag, info in tag_to_overall_count_and_names.items():\n",
        "    names_counts = ', '.join([f\"{name} - {count}\" for name, count in info[\"names\"].items()])\n",
        "    print(f\"{tag.capitalize()} - {names_counts}; Total - {info['count']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM8-6Dh2OOqD",
        "outputId": "617ce5dc-7dbf-476b-fcea-1652226acb21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarah - Sarah - 1; Total - 1\n",
            "Dedicated - Sarah - 1, Sophia - 1, Lily - 1; Total - 3\n",
            "Student - Sarah - 1, Mia - 1, Sophia - 1, Lily - 1, Max - 1; Total - 5\n",
            "Passion - Sarah - 1, Grace - 1, Charlotte - 1; Total - 3\n",
            "Mathematics - Sarah - 1; Total - 1\n",
            "Physic - Sarah - 1, Leo - 1; Total - 2\n",
            "Interest - Sarah - 1, James - 1, Lucas - 1; Total - 3\n",
            "Science - Alex - 1, James - 1, Mia - 1, Olivia - 1, Lily - 1, Zoe - 1, Isabella - 1, Gia - 1; Total - 8\n",
            "Alex - Alex - 1; Total - 1\n",
            "Tech - Alex - 1, Oliver - 1; Total - 2\n",
            "Enthusiast - Alex - 1, James - 1, Lucas - 1, Oliver - 1, William - 1; Total - 5\n",
            "Studying - Alex - 1, Lucas - 1, Oliver - 1; Total - 3\n",
            "Computer - Alex - 1, Max - 1, Oliver - 1; Total - 3\n",
            "Data - Alex - 1; Total - 1\n",
            "Creative - Emily - 1, Charlotte - 1; Total - 2\n",
            "Emily - Emily - 1; Total - 1\n",
            "Avid - Emily - 1; Total - 1\n",
            "Reader - Emily - 1; Total - 1\n",
            "Historian - Emily - 1; Total - 1\n",
            "Focusing - Emily - 1, Sophia - 1, Lily - 1; Total - 3\n",
            "Literature - Emily - 1, Charlotte - 1; Total - 2\n",
            "James - James - 1; Total - 1\n",
            "Excelling - James - 1; Total - 1\n",
            "Biology - James - 1, Ethan  - 1, Isabella - 1; Total - 3\n",
            "Chemistry - James - 1, Lily - 1; Total - 2\n",
            "Mia - Mia - 1; Total - 1\n",
            "Economics - Mia - 1, Noah - 1, Harper - 1; Total - 3\n",
            "Political - Mia - 1, Zoe - 1, Gia - 1; Total - 3\n",
            "Passionate - Mia - 1; Total - 1\n",
            "Understanding - Mia - 1; Total - 1\n",
            "Ryan - Ryan - 1; Total - 1\n",
            "Aspiring - Ryan - 1, Max - 1, Oliver - 1; Total - 3\n",
            "Engineer - Ryan - 1; Total - 1\n",
            "Fascinated - Ryan - 1; Total - 1\n",
            "Robotics - Ryan - 1, Mason - 1; Total - 2\n",
            "Renewable - Ryan - 1; Total - 1\n",
            "Energy - Ryan - 1; Total - 1\n",
            "Sophia - Sophia - 1; Total - 1\n",
            "Sociology - Sophia - 1; Total - 1\n",
            "Gender - Sophia - 1; Total - 1\n",
            "Study - Sophia - 1, Ava - 1, Leo - 1, Amelia - 1; Total - 4\n",
            "Art - Lucas - 1, Amelia - 1; Total - 2\n",
            "Lucas - Lucas - 1; Total - 1\n",
            "Fine - Lucas - 1, Amelia - 1; Total - 2\n",
            "Sculpture - Lucas - 1; Total - 1\n",
            "Genetic - Ethan  - 1; Total - 1\n",
            "Engineering - Ethan  - 1, Max - 1, Mason - 1, Benjamin - 1; Total - 4\n",
            "Biotechnology - Ethan  - 1; Total - 1\n",
            "Ethan - Ethan  - 1; Total - 1\n",
            "Academic - Ethan  - 1, Noah - 1, Benjamin - 1, Henry - 1; Total - 4\n",
            "Journey - Ethan  - 1; Total - 1\n",
            "Environmental - Olivia - 1, Lily - 1, Isabella - 1; Total - 3\n",
            "Olivia - Olivia - 1; Total - 1\n",
            "Budding - Olivia - 1; Total - 1\n",
            "Scientist - Olivia - 1; Total - 1\n",
            "Specializing - Olivia - 1; Total - 1\n",
            "Geology - Olivia - 1; Total - 1\n",
            "Lily - Lily - 1; Total - 1\n",
            "Ethical - Max - 1; Total - 1\n",
            "Hacking - Max - 1; Total - 1\n",
            "Max - Max - 1; Total - 1\n",
            "Ava - Ava - 1; Total - 1\n",
            "Delf - Ava - 1, Harper - 1, Amelia - 1, Isabella - 1, Gia - 1; Total - 5\n",
            "World - Ava - 1, Amelia - 1, Isabella - 1, Charlotte - 1; Total - 4\n",
            "Language - Ava - 1; Total - 1\n",
            "Culture - Ava - 1; Total - 1\n",
            "Linguistics - Ava - 1; Total - 1\n",
            "Financial - Noah - 1, Henry - 1; Total - 2\n",
            "Market - Noah - 1, Henry - 1; Total - 2\n",
            "Noah - Noah - 1; Total - 1\n",
            "Focus - Noah - 1, Grace - 1, Henry - 1; Total - 3\n",
            "Finance - Noah - 1, Henry - 1; Total - 2\n",
            "International - Zoe - 1; Total - 1\n",
            "Diplomacy - Zoe - 1; Total - 1\n",
            "Global - Zoe - 1; Total - 1\n",
            "Zoe - Zoe - 1; Total - 1\n",
            "Immerses - Zoe - 1, Charlotte - 1; Total - 2\n",
            "Leo - Leo - 1; Total - 1\n",
            "Explores - Leo - 1; Total - 1\n",
            "Wonder - Leo - 1; Total - 1\n",
            "Universe - Leo - 1; Total - 1\n",
            "Astronomy - Leo - 1; Total - 1\n",
            "Psychology - Harper - 1; Total - 1\n",
            "Behavioral - Harper - 1; Total - 1\n",
            "Harper - Harper - 1; Total - 1\n",
            "Realm - Harper - 1, William - 1; Total - 2\n",
            "Exploring - Harper - 1, William - 1; Total - 2\n",
            "Mechanical - Mason - 1; Total - 1\n",
            "Automation - Mason - 1; Total - 1\n",
            "Mason - Mason - 1; Total - 1\n",
            "Pursuit - Mason - 1, Benjamin - 1; Total - 2\n",
            "Reflect - Mason - 1; Total - 1\n",
            "Amelia - Amelia - 1; Total - 1\n",
            "History - Amelia - 1, William - 1; Total - 2\n",
            "Software - Oliver - 1; Total - 1\n",
            "Oliver - Oliver - 1; Total - 1\n",
            "Marine - Isabella - 1; Total - 1\n",
            "Isabella - Isabella - 1; Total - 1\n",
            "Ancient - William - 1; Total - 1\n",
            "William - William - 1; Total - 1\n",
            "Archaeology - William - 1; Total - 1\n",
            "Health - Grace - 1; Total - 1\n",
            "Disease - Grace - 1; Total - 1\n",
            "Grace - Grace - 1; Total - 1\n",
            "Public - Grace - 1; Total - 1\n",
            "Epidemiology - Grace - 1; Total - 1\n",
            "Aerospace - Benjamin - 1; Total - 1\n",
            "Space - Benjamin - 1; Total - 1\n",
            "Exploration - Benjamin - 1; Total - 1\n",
            "Benjamin - Benjamin - 1; Total - 1\n",
            "Human - Gia - 1; Total - 1\n",
            "Right - Gia - 1; Total - 1\n",
            "Advocating - Gia - 1; Total - 1\n",
            "Gia - Gia - 1; Total - 1\n",
            "Risk - Henry - 1; Total - 1\n",
            "Henry - Henry - 1; Total - 1\n",
            "Writing - Charlotte - 1; Total - 1\n",
            "Charlotte - Charlotte - 1; Total - 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Solution - Custom Stopwords"
      ],
      "metadata": {
        "id": "-Hnp4Ejfemdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Assuming data is loaded into a DataFrame `data`\n",
        "data = pd.read_csv(\"/content/Final_pd.csv\")\n",
        "\n",
        "# Custom stopwords\n",
        "custom_stopwords = {'dedicated', 'life', 'passion', 'interest'}  # Extend as needed\n",
        "stop_words = set(stopwords.words('english')).union(custom_stopwords)\n",
        "\n",
        "# Function to tokenize, remove custom stopwords and verbs\n",
        "def tokenize_and_filter(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and lower case\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove custom stopwords\n",
        "    # POS tagging and filter out verbs (keep nouns, NN; you might also keep adjectives, JJ)\n",
        "    tagged = pos_tag(filtered_tokens)\n",
        "    nouns_adjectives = [word for word, tag in tagged if tag.startswith('NN') or tag.startswith('JJ')]\n",
        "    return ' '.join(nouns_adjectives)\n",
        "\n",
        "# Apply the function to the descriptions\n",
        "data['filtered_description'] = data['Description'].apply(tokenize_and_filter)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit TF-IDF on the filtered descriptions\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['filtered_description'])\n",
        "\n",
        "# Extract feature names to map back to words\n",
        "features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "### Step 3: Extracting Top 7 Tags\n",
        "\n",
        "def get_top_n_tags(row_data, features, n=7):\n",
        "    # Get indices sorted by value in descending order\n",
        "    sorted_indices = row_data.argsort()[-n:][::-1]\n",
        "    return [features[i] for i in sorted_indices]\n",
        "\n",
        "# Extract top 7 tags for each row\n",
        "data['Top_7_Tags'] = [get_top_n_tags(row, features) for row in tfidf_matrix.toarray()]\n",
        "\n",
        "### Step 4: Map Names to Tags and Count\n",
        "\n",
        "name_to_top_tags = pd.Series(data['Top_7_Tags'].values, index=data['Name']).to_dict()\n",
        "\n",
        "print(name_to_top_tags)\n",
        "\n",
        "tag_to_names = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "# Populate tag_to_names\n",
        "for name, tags in name_to_top_tags.items():\n",
        "    for tag in tags:\n",
        "        tag_to_names[tag][name] += 1\n",
        "\n",
        "# Consolidate counts and print\n",
        "for tag, names in tag_to_names.items():\n",
        "    count = sum(names.values())  # Total count for the tag\n",
        "    names_str = ', '.join([f\"{name} {names[name]}\" for name in names])\n",
        "    print(f\"{tag.capitalize()} - {names_str}; Total: {count}\")\n",
        "\n",
        "\n",
        "output_file_path = \"/content/Final_pd_with_top_tagged.csv\"\n",
        "data.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDn1uc3nPKds",
        "outputId": "20cb2f21-e6bd-4fcd-cb1d-f85a906e1b0e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Sarah': ['astrophysics', 'quantum', 'mechanics', 'curiosity', 'fervor', 'sarah', 'mathematics'], 'Alex': ['science', 'machine', 'learning', 'data', 'computing', 'intelligence', 'alex'], 'Emily': ['creative', 'deep', 'historian', 'avid', 'joy', 'narratives', 'reader'], 'James': ['experiments', 'molecular', 'unravel', 'james', 'level', 'chemistry', 'genetics'], 'Mia': ['policy', 'mia', 'positive', 'commitment', 'impact', 'society', 'passionate'], 'Ryan': ['projects', 'engineer', 'autonomous', 'friendly', 'renewable', 'eco', 'energy'], 'Sophia': ['structures', 'gender', 'intersectionality', 'equality', 'social', 'societal', 'sociology'], 'Lucas': ['art', 'explores', 'traditional', 'installation', 'sculpture', 'boundaries', 'inspire'], 'Ethan ': ['genetic', 'biotechnology', 'engineering', 'applications', 'manipulation', 'advancements', 'realm'], 'Olivia': ['environmental', 'scientist', 'natural', 'earth', 'propel', 'generations', 'geology'], 'Lily': ['chemistry', 'environmental', 'knowledge', 'challenges', 'address', 'sustainability', 'mitigation'], 'Max': ['ethical', 'hacking', 'secure', 'network', 'keen', 'max', 'contribute'], 'Ava': ['studies', 'ava', 'gaps', 'linguistic', 'bridge', 'communication', 'multilingualism'], 'Noah': ['financial', 'planning', 'precision', 'noah', 'economic', 'analysis', 'stock'], 'Zoe': ['international', 'global', 'zoe', 'affairs', 'cooperation', 'aspirations', 'governance'], 'Leo': ['objects', 'wonders', 'beauty', 'cosmology', 'inquiry', 'leo', 'astrophotography'], 'Harper': ['behavioral', 'psychology', 'economics', 'understand', 'harper', 'processes', 'cognitive'], 'Mason': ['mechanical', 'robotics', 'efficiency', 'mason', 'biomechanics', 'functionality', 'automation'], 'Amelia': ['art', 'media', 'mixed', 'modern', 'techniques', 'creativity', 'amelia'], 'Oliver': ['software', 'development', 'cybersecurity', 'tech', 'digital', 'security', 'solutions'], 'Isabella': ['marine', 'intricate', 'oceanography', 'conservation', 'isabella', 'dynamics', 'biology'], 'William': ['ancient', 'history', 'cultures', 'preservation', 'past', 'stories', 'archaeology'], 'Grace': ['health', 'dedication', 'diseases', 'worldwide', 'epidemiology', 'initiatives', 'prevention'], 'Benjamin': ['engineering', 'space', 'exploration', 'travel', 'aerodynamics', 'aerospace', 'benjamin'], 'Gia': ['rights', 'human', 'gia', 'scale', 'frameworks', 'legal', 'law'], 'Henry': ['financial', 'navigates', 'acumen', 'strategic', 'management', 'banking', 'henry'], 'Charlotte': ['creative', 'imagination', 'fiction', 'poetry', 'charlotte', 'words', 'literary']}\n",
            "Astrophysics - Sarah 1; Total: 1\n",
            "Quantum - Sarah 1; Total: 1\n",
            "Mechanics - Sarah 1; Total: 1\n",
            "Curiosity - Sarah 1; Total: 1\n",
            "Fervor - Sarah 1; Total: 1\n",
            "Sarah - Sarah 1; Total: 1\n",
            "Mathematics - Sarah 1; Total: 1\n",
            "Science - Alex 1; Total: 1\n",
            "Machine - Alex 1; Total: 1\n",
            "Learning - Alex 1; Total: 1\n",
            "Data - Alex 1; Total: 1\n",
            "Computing - Alex 1; Total: 1\n",
            "Intelligence - Alex 1; Total: 1\n",
            "Alex - Alex 1; Total: 1\n",
            "Creative - Emily 1, Charlotte 1; Total: 2\n",
            "Deep - Emily 1; Total: 1\n",
            "Historian - Emily 1; Total: 1\n",
            "Avid - Emily 1; Total: 1\n",
            "Joy - Emily 1; Total: 1\n",
            "Narratives - Emily 1; Total: 1\n",
            "Reader - Emily 1; Total: 1\n",
            "Experiments - James 1; Total: 1\n",
            "Molecular - James 1; Total: 1\n",
            "Unravel - James 1; Total: 1\n",
            "James - James 1; Total: 1\n",
            "Level - James 1; Total: 1\n",
            "Chemistry - James 1, Lily 1; Total: 2\n",
            "Genetics - James 1; Total: 1\n",
            "Policy - Mia 1; Total: 1\n",
            "Mia - Mia 1; Total: 1\n",
            "Positive - Mia 1; Total: 1\n",
            "Commitment - Mia 1; Total: 1\n",
            "Impact - Mia 1; Total: 1\n",
            "Society - Mia 1; Total: 1\n",
            "Passionate - Mia 1; Total: 1\n",
            "Projects - Ryan 1; Total: 1\n",
            "Engineer - Ryan 1; Total: 1\n",
            "Autonomous - Ryan 1; Total: 1\n",
            "Friendly - Ryan 1; Total: 1\n",
            "Renewable - Ryan 1; Total: 1\n",
            "Eco - Ryan 1; Total: 1\n",
            "Energy - Ryan 1; Total: 1\n",
            "Structures - Sophia 1; Total: 1\n",
            "Gender - Sophia 1; Total: 1\n",
            "Intersectionality - Sophia 1; Total: 1\n",
            "Equality - Sophia 1; Total: 1\n",
            "Social - Sophia 1; Total: 1\n",
            "Societal - Sophia 1; Total: 1\n",
            "Sociology - Sophia 1; Total: 1\n",
            "Art - Lucas 1, Amelia 1; Total: 2\n",
            "Explores - Lucas 1; Total: 1\n",
            "Traditional - Lucas 1; Total: 1\n",
            "Installation - Lucas 1; Total: 1\n",
            "Sculpture - Lucas 1; Total: 1\n",
            "Boundaries - Lucas 1; Total: 1\n",
            "Inspire - Lucas 1; Total: 1\n",
            "Genetic - Ethan  1; Total: 1\n",
            "Biotechnology - Ethan  1; Total: 1\n",
            "Engineering - Ethan  1, Benjamin 1; Total: 2\n",
            "Applications - Ethan  1; Total: 1\n",
            "Manipulation - Ethan  1; Total: 1\n",
            "Advancements - Ethan  1; Total: 1\n",
            "Realm - Ethan  1; Total: 1\n",
            "Environmental - Olivia 1, Lily 1; Total: 2\n",
            "Scientist - Olivia 1; Total: 1\n",
            "Natural - Olivia 1; Total: 1\n",
            "Earth - Olivia 1; Total: 1\n",
            "Propel - Olivia 1; Total: 1\n",
            "Generations - Olivia 1; Total: 1\n",
            "Geology - Olivia 1; Total: 1\n",
            "Knowledge - Lily 1; Total: 1\n",
            "Challenges - Lily 1; Total: 1\n",
            "Address - Lily 1; Total: 1\n",
            "Sustainability - Lily 1; Total: 1\n",
            "Mitigation - Lily 1; Total: 1\n",
            "Ethical - Max 1; Total: 1\n",
            "Hacking - Max 1; Total: 1\n",
            "Secure - Max 1; Total: 1\n",
            "Network - Max 1; Total: 1\n",
            "Keen - Max 1; Total: 1\n",
            "Max - Max 1; Total: 1\n",
            "Contribute - Max 1; Total: 1\n",
            "Studies - Ava 1; Total: 1\n",
            "Ava - Ava 1; Total: 1\n",
            "Gaps - Ava 1; Total: 1\n",
            "Linguistic - Ava 1; Total: 1\n",
            "Bridge - Ava 1; Total: 1\n",
            "Communication - Ava 1; Total: 1\n",
            "Multilingualism - Ava 1; Total: 1\n",
            "Financial - Noah 1, Henry 1; Total: 2\n",
            "Planning - Noah 1; Total: 1\n",
            "Precision - Noah 1; Total: 1\n",
            "Noah - Noah 1; Total: 1\n",
            "Economic - Noah 1; Total: 1\n",
            "Analysis - Noah 1; Total: 1\n",
            "Stock - Noah 1; Total: 1\n",
            "International - Zoe 1; Total: 1\n",
            "Global - Zoe 1; Total: 1\n",
            "Zoe - Zoe 1; Total: 1\n",
            "Affairs - Zoe 1; Total: 1\n",
            "Cooperation - Zoe 1; Total: 1\n",
            "Aspirations - Zoe 1; Total: 1\n",
            "Governance - Zoe 1; Total: 1\n",
            "Objects - Leo 1; Total: 1\n",
            "Wonders - Leo 1; Total: 1\n",
            "Beauty - Leo 1; Total: 1\n",
            "Cosmology - Leo 1; Total: 1\n",
            "Inquiry - Leo 1; Total: 1\n",
            "Leo - Leo 1; Total: 1\n",
            "Astrophotography - Leo 1; Total: 1\n",
            "Behavioral - Harper 1; Total: 1\n",
            "Psychology - Harper 1; Total: 1\n",
            "Economics - Harper 1; Total: 1\n",
            "Understand - Harper 1; Total: 1\n",
            "Harper - Harper 1; Total: 1\n",
            "Processes - Harper 1; Total: 1\n",
            "Cognitive - Harper 1; Total: 1\n",
            "Mechanical - Mason 1; Total: 1\n",
            "Robotics - Mason 1; Total: 1\n",
            "Efficiency - Mason 1; Total: 1\n",
            "Mason - Mason 1; Total: 1\n",
            "Biomechanics - Mason 1; Total: 1\n",
            "Functionality - Mason 1; Total: 1\n",
            "Automation - Mason 1; Total: 1\n",
            "Media - Amelia 1; Total: 1\n",
            "Mixed - Amelia 1; Total: 1\n",
            "Modern - Amelia 1; Total: 1\n",
            "Techniques - Amelia 1; Total: 1\n",
            "Creativity - Amelia 1; Total: 1\n",
            "Amelia - Amelia 1; Total: 1\n",
            "Software - Oliver 1; Total: 1\n",
            "Development - Oliver 1; Total: 1\n",
            "Cybersecurity - Oliver 1; Total: 1\n",
            "Tech - Oliver 1; Total: 1\n",
            "Digital - Oliver 1; Total: 1\n",
            "Security - Oliver 1; Total: 1\n",
            "Solutions - Oliver 1; Total: 1\n",
            "Marine - Isabella 1; Total: 1\n",
            "Intricate - Isabella 1; Total: 1\n",
            "Oceanography - Isabella 1; Total: 1\n",
            "Conservation - Isabella 1; Total: 1\n",
            "Isabella - Isabella 1; Total: 1\n",
            "Dynamics - Isabella 1; Total: 1\n",
            "Biology - Isabella 1; Total: 1\n",
            "Ancient - William 1; Total: 1\n",
            "History - William 1; Total: 1\n",
            "Cultures - William 1; Total: 1\n",
            "Preservation - William 1; Total: 1\n",
            "Past - William 1; Total: 1\n",
            "Stories - William 1; Total: 1\n",
            "Archaeology - William 1; Total: 1\n",
            "Health - Grace 1; Total: 1\n",
            "Dedication - Grace 1; Total: 1\n",
            "Diseases - Grace 1; Total: 1\n",
            "Worldwide - Grace 1; Total: 1\n",
            "Epidemiology - Grace 1; Total: 1\n",
            "Initiatives - Grace 1; Total: 1\n",
            "Prevention - Grace 1; Total: 1\n",
            "Space - Benjamin 1; Total: 1\n",
            "Exploration - Benjamin 1; Total: 1\n",
            "Travel - Benjamin 1; Total: 1\n",
            "Aerodynamics - Benjamin 1; Total: 1\n",
            "Aerospace - Benjamin 1; Total: 1\n",
            "Benjamin - Benjamin 1; Total: 1\n",
            "Rights - Gia 1; Total: 1\n",
            "Human - Gia 1; Total: 1\n",
            "Gia - Gia 1; Total: 1\n",
            "Scale - Gia 1; Total: 1\n",
            "Frameworks - Gia 1; Total: 1\n",
            "Legal - Gia 1; Total: 1\n",
            "Law - Gia 1; Total: 1\n",
            "Navigates - Henry 1; Total: 1\n",
            "Acumen - Henry 1; Total: 1\n",
            "Strategic - Henry 1; Total: 1\n",
            "Management - Henry 1; Total: 1\n",
            "Banking - Henry 1; Total: 1\n",
            "Henry - Henry 1; Total: 1\n",
            "Imagination - Charlotte 1; Total: 1\n",
            "Fiction - Charlotte 1; Total: 1\n",
            "Poetry - Charlotte 1; Total: 1\n",
            "Charlotte - Charlotte 1; Total: 1\n",
            "Words - Charlotte 1; Total: 1\n",
            "Literary - Charlotte 1; Total: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Solutions - implemented tf-id vectorisations\n"
      ],
      "metadata": {
        "id": "oMRdaR8dZukU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"/content/Final_pd.csv\")\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Filter out stopwords, verbs, and adjectives\n",
        "    filtered_tokens = [word for word in tokens if word not in stopwords.words('english') and pos_tag([word])[0][1] not in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ']]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Preprocess descriptions\n",
        "data['processed_description'] = data['Description'].apply(preprocess_text)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_description'])\n",
        "features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Function to extract top 7 tags\n",
        "def extract_top_tags(row_data, features, n=7):\n",
        "    sorted_indices = row_data.argsort()[-n:][::-1]\n",
        "    return [features[i] for i in sorted_indices]\n",
        "\n",
        "# Extract top tags for each row\n",
        "data['top_tags'] = [extract_top_tags(row, features) for row in tfidf_matrix.toarray()]\n",
        "\n",
        "# Update CSV with tags and ranks\n",
        "for i, tags in enumerate(data['top_tags']):\n",
        "    for j, tag in enumerate(tags):\n",
        "        data.at[i, f\"Tag{j+1}\"] = tag\n",
        "\n",
        "# Map names to tags\n",
        "name_to_tags = defaultdict(list)\n",
        "for idx, row in data.iterrows():\n",
        "    name = row['Name']\n",
        "    tags = row['top_tags']\n",
        "    for tag in tags:\n",
        "        name_to_tags[name].append(tag)\n",
        "\n",
        "# Count occurrences of each tag across all names\n",
        "tag_to_names = defaultdict(lambda: defaultdict(int))\n",
        "for name, tags in name_to_tags.items():\n",
        "    for tag in tags:\n",
        "        tag_to_names[tag][name] += 1\n",
        "\n",
        "# Print tag occurrences and associated names\n",
        "for tag, names in tag_to_names.items():\n",
        "    count = sum(names.values())\n",
        "    names_str = ', '.join([f\"{name} - {count}\" for name, count in names.items()])\n",
        "    print(f\"{tag.capitalize()} - {names_str}; Total: {count}\")\n",
        "\n",
        "# Function to search for tags and update the dictionary\n",
        "def search_tags(tag):\n",
        "    tag_dict = defaultdict(list)\n",
        "    for name, tags in name_to_tags.items():\n",
        "        if tag in tags:\n",
        "            tag_dict[tag].append(name)\n",
        "    return tag_dict\n",
        "\n",
        "# Example: Search for a tag\n",
        "tag_to_search = \"Physics\"\n",
        "search_result = search_tags(tag_to_search)\n",
        "print(f\"Search Result for Tag '{tag_to_search}': {search_result}\")\n",
        "\n",
        "# Save updated CSV\n",
        "data.to_csv(\"/content/updated_csv_file.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPys8GSdD0_R",
        "outputId": "1039fd4e-95cd-481b-bbf3-d307f889fac0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarah - Sarah - 1; Total: 1\n",
            "Curiosity - Sarah - 1; Total: 1\n",
            "Astrophysics - Sarah - 1; Total: 1\n",
            "Quantum - Sarah - 1; Total: 1\n",
            "Mathematics - Sarah - 1; Total: 1\n",
            "Fervor - Sarah - 1; Total: 1\n",
            "Mechanics - Sarah - 1; Total: 1\n",
            "Science - Alex - 1; Total: 1\n",
            "Around - Alex - 1; Total: 1\n",
            "Alex - Alex - 1; Total: 1\n",
            "Data - Alex - 1; Total: 1\n",
            "Machine - Alex - 1; Total: 1\n",
            "Revolve - Alex - 1; Total: 1\n",
            "Intelligence - Alex - 1; Total: 1\n",
            "Deep - Emily - 1; Total: 1\n",
            "Historian - Emily - 1; Total: 1\n",
            "Narratives - Emily - 1; Total: 1\n",
            "Reader - Emily - 1; Total: 1\n",
            "Emily - Emily - 1; Total: 1\n",
            "Avid - Emily - 1; Total: 1\n",
            "Joy - Emily - 1; Total: 1\n",
            "Biochemistry - James - 1; Total: 1\n",
            "Unravel - James - 1; Total: 1\n",
            "Experiments - James - 1; Total: 1\n",
            "Level - James - 1; Total: 1\n",
            "Molecular - James - 1; Total: 1\n",
            "James - James - 1; Total: 1\n",
            "Chemistry - James - 1, Lily - 1; Total: 2\n",
            "Policy - Mia - 1; Total: 1\n",
            "Society - Mia - 1; Total: 1\n",
            "Mia - Mia - 1; Total: 1\n",
            "Impact - Mia - 1; Total: 1\n",
            "Commitment - Mia - 1; Total: 1\n",
            "Relations - Mia - 1; Total: 1\n",
            "Dynamics - Mia - 1; Total: 1\n",
            "Create - Ryan - 1; Total: 1\n",
            "Friendly - Ryan - 1; Total: 1\n",
            "Energy - Ryan - 1; Total: 1\n",
            "Ryan - Ryan - 1; Total: 1\n",
            "Efficient - Ryan - 1; Total: 1\n",
            "Eco - Ryan - 1; Total: 1\n",
            "Projects - Ryan - 1; Total: 1\n",
            "Mental - Sophia - 1; Total: 1\n",
            "Psychology - Sophia - 1, Harper - 1; Total: 2\n",
            "Studies - Sophia - 1, Ava - 1; Total: 2\n",
            "Being - Sophia - 1; Total: 1\n",
            "Health - Sophia - 1, Grace - 1; Total: 2\n",
            "Equality - Sophia - 2; Total: 2\n",
            "Behavior - Sophia - 1; Total: 1\n",
            "Structures - Sophia - 1; Total: 1\n",
            "Intersectionality - Sophia - 1; Total: 1\n",
            "Advocates - Sophia - 1; Total: 1\n",
            "Gender - Sophia - 1; Total: 1\n",
            "Societal - Sophia - 1; Total: 1\n",
            "Sociology - Sophia - 1; Total: 1\n",
            "Art - Lucas - 2, Amelia - 1; Total: 3\n",
            "Meanings - Lucas - 1; Total: 1\n",
            "Aficionado - Lucas - 1; Total: 1\n",
            "Inspire - Lucas - 2; Total: 2\n",
            "Sculpture - Lucas - 2; Total: 2\n",
            "Lucas - Lucas - 2; Total: 2\n",
            "Endeavors - Lucas - 1; Total: 1\n",
            "Challenge - Lucas - 1; Total: 1\n",
            "Installation - Lucas - 1; Total: 1\n",
            "Boundaries - Lucas - 1; Total: 1\n",
            "Language - Ethan  - 1; Total: 1\n",
            "Time - Ethan  - 1; Total: 1\n",
            "Evolution - Ethan  - 1; Total: 1\n",
            "Evolve - Ethan  - 1; Total: 1\n",
            "Societies - Ethan  - 1; Total: 1\n",
            "Communicate - Ethan  - 1; Total: 1\n",
            "Ways - Ethan  - 1; Total: 1\n",
            "Biotechnology - Ethan  - 1; Total: 1\n",
            "Engineering - Ethan  - 1, Benjamin - 1; Total: 2\n",
            "Applications - Ethan  - 1; Total: 1\n",
            "Advancements - Ethan  - 1; Total: 1\n",
            "Manipulation - Ethan  - 1; Total: 1\n",
            "Beyond - Ethan  - 1; Total: 1\n",
            "Leads - Ethan  - 1; Total: 1\n",
            "Geology - Olivia - 1; Total: 1\n",
            "Budding - Olivia - 1; Total: 1\n",
            "Generations - Olivia - 1; Total: 1\n",
            "Olivia - Olivia - 1; Total: 1\n",
            "Scientist - Olivia - 1; Total: 1\n",
            "Mapping - Olivia - 1; Total: 1\n",
            "Earth - Olivia - 1; Total: 1\n",
            "Lily - Lily - 1; Total: 1\n",
            "Mitigation - Lily - 1; Total: 1\n",
            "Knowledge - Lily - 1; Total: 1\n",
            "Address - Lily - 1; Total: 1\n",
            "Challenges - Lily - 1; Total: 1\n",
            "Sustainability - Lily - 1; Total: 1\n",
            "Practices - Max - 1; Total: 1\n",
            "Max - Max - 1; Total: 1\n",
            "Safer - Max - 1; Total: 1\n",
            "Network - Max - 1; Total: 1\n",
            "Keen - Max - 1; Total: 1\n",
            "Secure - Max - 1; Total: 1\n",
            "Drives - Max - 1; Total: 1\n",
            "Aims - Ava - 1; Total: 1\n",
            "Gaps - Ava - 1; Total: 1\n",
            "Foster - Ava - 1; Total: 1\n",
            "Multilingualism - Ava - 1; Total: 1\n",
            "Communication - Ava - 1; Total: 1\n",
            "Bridge - Ava - 1; Total: 1\n",
            "Analysis - Noah - 1; Total: 1\n",
            "Stock - Noah - 1; Total: 1\n",
            "Noah - Noah - 1; Total: 1\n",
            "Planning - Noah - 1; Total: 1\n",
            "Precision - Noah - 1; Total: 1\n",
            "Intricacies - Noah - 1; Total: 1\n",
            "Finance - Noah - 1; Total: 1\n",
            "Diplomacy - Zoe - 1; Total: 1\n",
            "Zoe - Zoe - 1; Total: 1\n",
            "Peace - Zoe - 1; Total: 1\n",
            "Governance - Zoe - 1; Total: 1\n",
            "Cooperation - Zoe - 1; Total: 1\n",
            "Driven - Zoe - 1; Total: 1\n",
            "Affairs - Zoe - 1; Total: 1\n",
            "Cosmology - Leo - 1; Total: 1\n",
            "Leo - Leo - 1; Total: 1\n",
            "Astrophotography - Leo - 1; Total: 1\n",
            "Beauty - Leo - 1; Total: 1\n",
            "Inquiry - Leo - 1; Total: 1\n",
            "Cosmos - Leo - 1; Total: 1\n",
            "Astronomy - Leo - 1; Total: 1\n",
            "Economics - Harper - 1; Total: 1\n",
            "Cognitive - Harper - 1; Total: 1\n",
            "Individuals - Harper - 1; Total: 1\n",
            "Choices - Harper - 1; Total: 1\n",
            "Processes - Harper - 1; Total: 1\n",
            "Harper - Harper - 1; Total: 1\n",
            "Automation - Mason - 1; Total: 1\n",
            "Robotics - Mason - 1; Total: 1\n",
            "Enhance - Mason - 1; Total: 1\n",
            "Functionality - Mason - 1; Total: 1\n",
            "Mason - Mason - 1; Total: 1\n",
            "Efficiency - Mason - 1; Total: 1\n",
            "Biomechanics - Mason - 1; Total: 1\n",
            "Creativity - Amelia - 1; Total: 1\n",
            "Amelia - Amelia - 1; Total: 1\n",
            "Expresses - Amelia - 1; Total: 1\n",
            "Media - Amelia - 1; Total: 1\n",
            "Movements - Amelia - 1; Total: 1\n",
            "Techniques - Amelia - 1; Total: 1\n",
            "Software - Oliver - 1; Total: 1\n",
            "Oliver - Oliver - 1; Total: 1\n",
            "App - Oliver - 1; Total: 1\n",
            "Development - Oliver - 1; Total: 1\n",
            "Cybersecurity - Oliver - 1; Total: 1\n",
            "Security - Oliver - 1; Total: 1\n",
            "Solutions - Oliver - 1; Total: 1\n",
            "Marine - Isabella - 1; Total: 1\n",
            "Isabella - Isabella - 1; Total: 1\n",
            "Oceanography - Isabella - 1; Total: 1\n",
            "Ocean - Isabella - 1; Total: 1\n",
            "Intricate - Isabella - 1; Total: 1\n",
            "Ecosystems - Isabella - 1; Total: 1\n",
            "Conservation - Isabella - 1; Total: 1\n",
            "Ancient - William - 1; Total: 1\n",
            "History - William - 1; Total: 1\n",
            "Preservation - William - 1; Total: 1\n",
            "Stories - William - 1; Total: 1\n",
            "Artifacts - William - 1; Total: 1\n",
            "Archaeology - William - 1; Total: 1\n",
            "Shape - William - 1; Total: 1\n",
            "Disease - Grace - 1; Total: 1\n",
            "Epidemiology - Grace - 1; Total: 1\n",
            "Initiatives - Grace - 1; Total: 1\n",
            "Diseases - Grace - 1; Total: 1\n",
            "Prevention - Grace - 1; Total: 1\n",
            "Worldwide - Grace - 1; Total: 1\n",
            "Space - Benjamin - 1; Total: 1\n",
            "Aerospace - Benjamin - 1; Total: 1\n",
            "Exploration - Benjamin - 1; Total: 1\n",
            "Benjamin - Benjamin - 1; Total: 1\n",
            "Travel - Benjamin - 1; Total: 1\n",
            "Aerodynamics - Benjamin - 1; Total: 1\n",
            "Rights - Gia - 1; Total: 1\n",
            "Human - Gia - 1; Total: 1\n",
            "Frameworks - Gia - 1; Total: 1\n",
            "Scale - Gia - 1; Total: 1\n",
            "Gia - Gia - 1; Total: 1\n",
            "Law - Gia - 1; Total: 1\n",
            "Fuels - Gia - 1; Total: 1\n",
            "Management - Henry - 1; Total: 1\n",
            "Risk - Henry - 1; Total: 1\n",
            "Banking - Henry - 1; Total: 1\n",
            "Henry - Henry - 1; Total: 1\n",
            "Investment - Henry - 1; Total: 1\n",
            "Risks - Henry - 1; Total: 1\n",
            "Acumen - Henry - 1; Total: 1\n",
            "Power - Charlotte - 1; Total: 1\n",
            "Words - Charlotte - 1; Total: 1\n",
            "Charlotte - Charlotte - 1; Total: 1\n",
            "Fiction - Charlotte - 1; Total: 1\n",
            "Imagination - Charlotte - 1; Total: 1\n",
            "Poetry - Charlotte - 1; Total: 1\n",
            "Literature - Charlotte - 1; Total: 1\n",
            "Search Result for Tag 'Physics': defaultdict(<class 'list'>, {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4th Solutions - Implementaed Spacy with tf-id\n"
      ],
      "metadata": {
        "id": "43IJO0vvdQGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "file_path = \"/content/Final_pd.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to preprocess text and extract features using TF-IDF\n",
        "def preprocess_and_extract_features(text):\n",
        "    # Tokenize text and filter out stopwords and irrelevant words\n",
        "    tokens = [token.text for token in nlp(text) if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Preprocess descriptions and extract features using TF-IDF\n",
        "data['processed_description'] = data['Description'].apply(preprocess_and_extract_features)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_description'])\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Map names to their top 7 tags with weights\n",
        "name_to_top_tags = defaultdict(lambda: defaultdict(float))\n",
        "for i, row in data.iterrows():\n",
        "    doc_weights = tfidf_matrix[i].toarray()[0]\n",
        "    top_indices = doc_weights.argsort()[-7:][::-1]\n",
        "    for idx in top_indices:\n",
        "        name_to_top_tags[row['Name']][feature_names[idx]] = doc_weights[idx]\n",
        "\n",
        "# Map tags to names and count occurrences\n",
        "tag_to_names = defaultdict(lambda: defaultdict(int))\n",
        "for name, tags in name_to_top_tags.items():\n",
        "    for tag, weight in tags.items():\n",
        "        tag_to_names[tag][name] += 1\n",
        "\n",
        "# Update the DataFrame with top tags and weights\n",
        "top_tags = []\n",
        "tag_weights = []\n",
        "for index, row in data.iterrows():\n",
        "    tags = list(name_to_top_tags[row['Name']].keys())\n",
        "    weights = list(name_to_top_tags[row['Name']].values())\n",
        "    if len(tags) < 7:\n",
        "        tags += [''] * (7 - len(tags))  # Fill missing tags with empty strings\n",
        "        weights += [0.0] * (7 - len(weights))  # Fill missing weights with zeros\n",
        "    top_tags.append(tags)\n",
        "    tag_weights.append(weights)\n",
        "\n",
        "data['Top_7_Tags'] = top_tags\n",
        "data['Tag_Weights'] = tag_weights\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "new_file_path = \"/content/New_Final_pd1.csv\"\n",
        "data.to_csv(new_file_path, index=False)\n",
        "\n",
        "# Print tag-to-names mapping with occurrences\n",
        "for tag, names in tag_to_names.items():\n",
        "    count = sum(names.values())\n",
        "    print(f\"{tag.capitalize()} - {names} - Total: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90uyDUJuMtpf",
        "outputId": "bea1f0fe-3d6f-4e97-fbaa-668bea7f196e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mechanics - defaultdict(<class 'int'>, {'Sarah': 1}) - Total: 1\n",
            "Quantum - defaultdict(<class 'int'>, {'Sarah': 1}) - Total: 1\n",
            "Curiosity - defaultdict(<class 'int'>, {'Sarah': 1}) - Total: 1\n",
            "Sarah - defaultdict(<class 'int'>, {'Sarah': 1}) - Total: 1\n",
            "Fervor - defaultdict(<class 'int'>, {'Sarah': 1}) - Total: 1\n",
            "Astrophysics - defaultdict(<class 'int'>, {'Sarah': 1}) - Total: 1\n",
            "Mathematics - defaultdict(<class 'int'>, {'Sarah': 1}) - Total: 1\n",
            "Science - defaultdict(<class 'int'>, {'Alex': 1}) - Total: 1\n",
            "Machine - defaultdict(<class 'int'>, {'Alex': 1}) - Total: 1\n",
            "Revolve - defaultdict(<class 'int'>, {'Alex': 1}) - Total: 1\n",
            "Data - defaultdict(<class 'int'>, {'Alex': 1}) - Total: 1\n",
            "Artificial - defaultdict(<class 'int'>, {'Alex': 1}) - Total: 1\n",
            "Computing - defaultdict(<class 'int'>, {'Alex': 1}) - Total: 1\n",
            "Learning - defaultdict(<class 'int'>, {'Alex': 1}) - Total: 1\n",
            "Creative - defaultdict(<class 'int'>, {'Emily': 1, 'Charlotte': 1}) - Total: 2\n",
            "Historian - defaultdict(<class 'int'>, {'Emily': 1}) - Total: 1\n",
            "Reader - defaultdict(<class 'int'>, {'Emily': 1}) - Total: 1\n",
            "Finds - defaultdict(<class 'int'>, {'Emily': 1}) - Total: 1\n",
            "Avid - defaultdict(<class 'int'>, {'Emily': 1}) - Total: 1\n",
            "Inspiration - defaultdict(<class 'int'>, {'Emily': 1}) - Total: 1\n",
            "Deep - defaultdict(<class 'int'>, {'Emily': 1}) - Total: 1\n",
            "Molecular - defaultdict(<class 'int'>, {'James': 1}) - Total: 1\n",
            "Excelling - defaultdict(<class 'int'>, {'James': 1}) - Total: 1\n",
            "Experiments - defaultdict(<class 'int'>, {'James': 1}) - Total: 1\n",
            "Biochemistry - defaultdict(<class 'int'>, {'James': 1}) - Total: 1\n",
            "Unravel - defaultdict(<class 'int'>, {'James': 1}) - Total: 1\n",
            "Level - defaultdict(<class 'int'>, {'James': 1}) - Total: 1\n",
            "James - defaultdict(<class 'int'>, {'James': 1}) - Total: 1\n",
            "Making - defaultdict(<class 'int'>, {'Mia': 1}) - Total: 1\n",
            "Mia - defaultdict(<class 'int'>, {'Mia': 1}) - Total: 1\n",
            "Informed - defaultdict(<class 'int'>, {'Mia': 1}) - Total: 1\n",
            "Society - defaultdict(<class 'int'>, {'Mia': 1}) - Total: 1\n",
            "Policy - defaultdict(<class 'int'>, {'Mia': 1}) - Total: 1\n",
            "Impact - defaultdict(<class 'int'>, {'Mia': 1}) - Total: 1\n",
            "Positive - defaultdict(<class 'int'>, {'Mia': 1}) - Total: 1\n",
            "Friendly - defaultdict(<class 'int'>, {'Ryan': 1}) - Total: 1\n",
            "Energy - defaultdict(<class 'int'>, {'Ryan': 1}) - Total: 1\n",
            "Ryan - defaultdict(<class 'int'>, {'Ryan': 1}) - Total: 1\n",
            "Efficient - defaultdict(<class 'int'>, {'Ryan': 1}) - Total: 1\n",
            "Eco - defaultdict(<class 'int'>, {'Ryan': 1}) - Total: 1\n",
            "Autonomous - defaultdict(<class 'int'>, {'Ryan': 1}) - Total: 1\n",
            "Renewable - defaultdict(<class 'int'>, {'Ryan': 1}) - Total: 1\n",
            "Mental - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Psychology - defaultdict(<class 'int'>, {'Sophia': 1, 'Harper': 1}) - Total: 2\n",
            "Studies - defaultdict(<class 'int'>, {'Sophia': 1, 'Ava': 1}) - Total: 2\n",
            "Health - defaultdict(<class 'int'>, {'Sophia': 1, 'Grace': 1}) - Total: 2\n",
            "Fuel - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Equality - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Advocacy - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Gender - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Advocates - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Structures - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Intersectionality - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Social - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Societal - defaultdict(<class 'int'>, {'Sophia': 1}) - Total: 1\n",
            "Art - defaultdict(<class 'int'>, {'Lucas': 1, 'Amelia': 1}) - Total: 2\n",
            "Meanings - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Aficionado - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Lucas - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Sculpture - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Artistic - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Inspire - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Challenge - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Traditional - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Installation - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Boundaries - defaultdict(<class 'int'>, {'Lucas': 1}) - Total: 1\n",
            "Language - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Evolution - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Diversity - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Evolve - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Societies - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Communicate - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Ways - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Genetic - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Biotechnology - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Engineering - defaultdict(<class 'int'>, {'Ethan ': 1, 'Benjamin': 1}) - Total: 2\n",
            "Advancements - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Realm - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Manipulation - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Leads - defaultdict(<class 'int'>, {'Ethan ': 1}) - Total: 1\n",
            "Environmental - defaultdict(<class 'int'>, {'Olivia': 1, 'Lily': 1}) - Total: 2\n",
            "Earth - defaultdict(<class 'int'>, {'Olivia': 1}) - Total: 1\n",
            "Scientist - defaultdict(<class 'int'>, {'Olivia': 1}) - Total: 1\n",
            "Specializing - defaultdict(<class 'int'>, {'Olivia': 1}) - Total: 1\n",
            "Natural - defaultdict(<class 'int'>, {'Olivia': 1}) - Total: 1\n",
            "Generations - defaultdict(<class 'int'>, {'Olivia': 1}) - Total: 1\n",
            "Mapping - defaultdict(<class 'int'>, {'Olivia': 1}) - Total: 1\n",
            "Chemistry - defaultdict(<class 'int'>, {'Lily': 1}) - Total: 1\n",
            "Lily - defaultdict(<class 'int'>, {'Lily': 1}) - Total: 1\n",
            "Apply - defaultdict(<class 'int'>, {'Lily': 1}) - Total: 1\n",
            "Mitigation - defaultdict(<class 'int'>, {'Lily': 1}) - Total: 1\n",
            "Address - defaultdict(<class 'int'>, {'Lily': 1}) - Total: 1\n",
            "Promote - defaultdict(<class 'int'>, {'Lily': 1}) - Total: 1\n",
            "Hacking - defaultdict(<class 'int'>, {'Max': 1}) - Total: 1\n",
            "Ethical - defaultdict(<class 'int'>, {'Max': 1}) - Total: 1\n",
            "Secure - defaultdict(<class 'int'>, {'Max': 1}) - Total: 1\n",
            "Max - defaultdict(<class 'int'>, {'Max': 1}) - Total: 1\n",
            "Keen - defaultdict(<class 'int'>, {'Max': 1}) - Total: 1\n",
            "Safer - defaultdict(<class 'int'>, {'Max': 1}) - Total: 1\n",
            "Network - defaultdict(<class 'int'>, {'Max': 1}) - Total: 1\n",
            "Bridge - defaultdict(<class 'int'>, {'Ava': 1}) - Total: 1\n",
            "Communication - defaultdict(<class 'int'>, {'Ava': 1}) - Total: 1\n",
            "Translation - defaultdict(<class 'int'>, {'Ava': 1}) - Total: 1\n",
            "Aims - defaultdict(<class 'int'>, {'Ava': 1}) - Total: 1\n",
            "Ava - defaultdict(<class 'int'>, {'Ava': 1}) - Total: 1\n",
            "Linguistic - defaultdict(<class 'int'>, {'Ava': 1}) - Total: 1\n",
            "Financial - defaultdict(<class 'int'>, {'Noah': 1, 'Henry': 1}) - Total: 2\n",
            "Planning - defaultdict(<class 'int'>, {'Noah': 1}) - Total: 1\n",
            "Precision - defaultdict(<class 'int'>, {'Noah': 1}) - Total: 1\n",
            "Noah - defaultdict(<class 'int'>, {'Noah': 1}) - Total: 1\n",
            "Analysis - defaultdict(<class 'int'>, {'Noah': 1}) - Total: 1\n",
            "Economic - defaultdict(<class 'int'>, {'Noah': 1}) - Total: 1\n",
            "Stock - defaultdict(<class 'int'>, {'Noah': 1}) - Total: 1\n",
            "Diplomacy - defaultdict(<class 'int'>, {'Zoe': 1}) - Total: 1\n",
            "International - defaultdict(<class 'int'>, {'Zoe': 1}) - Total: 1\n",
            "Global - defaultdict(<class 'int'>, {'Zoe': 1}) - Total: 1\n",
            "Zoe - defaultdict(<class 'int'>, {'Zoe': 1}) - Total: 1\n",
            "Building - defaultdict(<class 'int'>, {'Zoe': 1}) - Total: 1\n",
            "Peace - defaultdict(<class 'int'>, {'Zoe': 1}) - Total: 1\n",
            "Efforts - defaultdict(<class 'int'>, {'Zoe': 1}) - Total: 1\n",
            "Inquiry - defaultdict(<class 'int'>, {'Leo': 1}) - Total: 1\n",
            "Captures - defaultdict(<class 'int'>, {'Leo': 1}) - Total: 1\n",
            "Scientific - defaultdict(<class 'int'>, {'Leo': 1}) - Total: 1\n",
            "Cosmology - defaultdict(<class 'int'>, {'Leo': 1}) - Total: 1\n",
            "Wonders - defaultdict(<class 'int'>, {'Leo': 1}) - Total: 1\n",
            "Objects - defaultdict(<class 'int'>, {'Leo': 1}) - Total: 1\n",
            "Celestial - defaultdict(<class 'int'>, {'Leo': 1}) - Total: 1\n",
            "Behavioral - defaultdict(<class 'int'>, {'Harper': 1}) - Total: 1\n",
            "Economics - defaultdict(<class 'int'>, {'Harper': 1}) - Total: 1\n",
            "Contexts - defaultdict(<class 'int'>, {'Harper': 1}) - Total: 1\n",
            "Processes - defaultdict(<class 'int'>, {'Harper': 1}) - Total: 1\n",
            "Choices - defaultdict(<class 'int'>, {'Harper': 1}) - Total: 1\n",
            "Individuals - defaultdict(<class 'int'>, {'Harper': 1}) - Total: 1\n",
            "Automation - defaultdict(<class 'int'>, {'Mason': 1}) - Total: 1\n",
            "Mechanical - defaultdict(<class 'int'>, {'Mason': 1}) - Total: 1\n",
            "Robotics - defaultdict(<class 'int'>, {'Mason': 1}) - Total: 1\n",
            "Biomechanics - defaultdict(<class 'int'>, {'Mason': 1}) - Total: 1\n",
            "Mason - defaultdict(<class 'int'>, {'Mason': 1}) - Total: 1\n",
            "Enhance - defaultdict(<class 'int'>, {'Mason': 1}) - Total: 1\n",
            "Functionality - defaultdict(<class 'int'>, {'Mason': 1}) - Total: 1\n",
            "Modern - defaultdict(<class 'int'>, {'Amelia': 1}) - Total: 1\n",
            "Movements - defaultdict(<class 'int'>, {'Amelia': 1}) - Total: 1\n",
            "Mixed - defaultdict(<class 'int'>, {'Amelia': 1}) - Total: 1\n",
            "Amelia - defaultdict(<class 'int'>, {'Amelia': 1}) - Total: 1\n",
            "Expresses - defaultdict(<class 'int'>, {'Amelia': 1}) - Total: 1\n",
            "Media - defaultdict(<class 'int'>, {'Amelia': 1}) - Total: 1\n",
            "Software - defaultdict(<class 'int'>, {'Oliver': 1}) - Total: 1\n",
            "App - defaultdict(<class 'int'>, {'Oliver': 1}) - Total: 1\n",
            "Creating - defaultdict(<class 'int'>, {'Oliver': 1}) - Total: 1\n",
            "Oliver - defaultdict(<class 'int'>, {'Oliver': 1}) - Total: 1\n",
            "Ensuring - defaultdict(<class 'int'>, {'Oliver': 1}) - Total: 1\n",
            "Development - defaultdict(<class 'int'>, {'Oliver': 1}) - Total: 1\n",
            "Digital - defaultdict(<class 'int'>, {'Oliver': 1}) - Total: 1\n",
            "Marine - defaultdict(<class 'int'>, {'Isabella': 1}) - Total: 1\n",
            "Isabella - defaultdict(<class 'int'>, {'Isabella': 1}) - Total: 1\n",
            "Conservation - defaultdict(<class 'int'>, {'Isabella': 1}) - Total: 1\n",
            "Intricate - defaultdict(<class 'int'>, {'Isabella': 1}) - Total: 1\n",
            "Committed - defaultdict(<class 'int'>, {'Isabella': 1}) - Total: 1\n",
            "Protecting - defaultdict(<class 'int'>, {'Isabella': 1}) - Total: 1\n",
            "Ecosystems - defaultdict(<class 'int'>, {'Isabella': 1}) - Total: 1\n",
            "Ancient - defaultdict(<class 'int'>, {'William': 1}) - Total: 1\n",
            "History - defaultdict(<class 'int'>, {'William': 1}) - Total: 1\n",
            "Shape - defaultdict(<class 'int'>, {'William': 1}) - Total: 1\n",
            "Uncovering - defaultdict(<class 'int'>, {'William': 1}) - Total: 1\n",
            "Stories - defaultdict(<class 'int'>, {'William': 1}) - Total: 1\n",
            "Historical - defaultdict(<class 'int'>, {'William': 1}) - Total: 1\n",
            "William - defaultdict(<class 'int'>, {'William': 1}) - Total: 1\n",
            "Improving - defaultdict(<class 'int'>, {'Grace': 1}) - Total: 1\n",
            "Focuses - defaultdict(<class 'int'>, {'Grace': 1}) - Total: 1\n",
            "Diseases - defaultdict(<class 'int'>, {'Grace': 1}) - Total: 1\n",
            "Disease - defaultdict(<class 'int'>, {'Grace': 1}) - Total: 1\n",
            "Prevention - defaultdict(<class 'int'>, {'Grace': 1}) - Total: 1\n",
            "Preventing - defaultdict(<class 'int'>, {'Grace': 1}) - Total: 1\n",
            "Space - defaultdict(<class 'int'>, {'Benjamin': 1}) - Total: 1\n",
            "Aerospace - defaultdict(<class 'int'>, {'Benjamin': 1}) - Total: 1\n",
            "Exploration - defaultdict(<class 'int'>, {'Benjamin': 1}) - Total: 1\n",
            "Pushing - defaultdict(<class 'int'>, {'Benjamin': 1}) - Total: 1\n",
            "Travel - defaultdict(<class 'int'>, {'Benjamin': 1}) - Total: 1\n",
            "Aerodynamics - defaultdict(<class 'int'>, {'Benjamin': 1}) - Total: 1\n",
            "Advocating - defaultdict(<class 'int'>, {'Gia': 1}) - Total: 1\n",
            "Rights - defaultdict(<class 'int'>, {'Gia': 1}) - Total: 1\n",
            "Human - defaultdict(<class 'int'>, {'Gia': 1}) - Total: 1\n",
            "Marginalized - defaultdict(<class 'int'>, {'Gia': 1}) - Total: 1\n",
            "Fuels - defaultdict(<class 'int'>, {'Gia': 1}) - Total: 1\n",
            "Law - defaultdict(<class 'int'>, {'Gia': 1}) - Total: 1\n",
            "Scale - defaultdict(<class 'int'>, {'Gia': 1}) - Total: 1\n",
            "Investment - defaultdict(<class 'int'>, {'Henry': 1}) - Total: 1\n",
            "Acumen - defaultdict(<class 'int'>, {'Henry': 1}) - Total: 1\n",
            "Strategic - defaultdict(<class 'int'>, {'Henry': 1}) - Total: 1\n",
            "Henry - defaultdict(<class 'int'>, {'Henry': 1}) - Total: 1\n",
            "Risks - defaultdict(<class 'int'>, {'Henry': 1}) - Total: 1\n",
            "Risk - defaultdict(<class 'int'>, {'Henry': 1}) - Total: 1\n",
            "Writing - defaultdict(<class 'int'>, {'Charlotte': 1}) - Total: 1\n",
            "Imagination - defaultdict(<class 'int'>, {'Charlotte': 1}) - Total: 1\n",
            "Storytelling - defaultdict(<class 'int'>, {'Charlotte': 1}) - Total: 1\n",
            "Fiction - defaultdict(<class 'int'>, {'Charlotte': 1}) - Total: 1\n",
            "Literary - defaultdict(<class 'int'>, {'Charlotte': 1}) - Total: 1\n",
            "Power - defaultdict(<class 'int'>, {'Charlotte': 1}) - Total: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. COMBINING TWO APPROACHES\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SGewkUHLar5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"/content/Final_pd.csv\")\n",
        "\n",
        "# Initialize NLTK tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_and_tokenize(text):\n",
        "    # Tokenize text using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Filter out stopwords, punctuation, and lemmatize tokens\n",
        "    tokens = [token.lemma_ for token in doc if token.text.lower() not in stop_words and token.text not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "df['Processed'] = df['cleaned_lemmatized_description'].apply(preprocess_and_tokenize)\n",
        "\n",
        "def score_terms(tokens):\n",
        "    # Count term frequencies\n",
        "    term_freq = Counter(tokens)\n",
        "    # Sort terms by frequency\n",
        "    sorted_terms = dict(sorted(term_freq.items(), key=lambda item: item[1], reverse=True))\n",
        "    return sorted_terms\n",
        "\n",
        "# Score and rank terms in descriptions\n",
        "df['Ranked_Terms'] = df['Processed'].apply(score_terms)\n",
        "\n",
        "def select_top_n_tags(ranked_terms, n=7):\n",
        "    # Select the top N terms\n",
        "    return list(ranked_terms.keys())[:n]\n",
        "\n",
        "# Select top 7 tags for each description\n",
        "df['Top_7_Tags'] = df['Ranked_Terms'].apply(lambda x: select_top_n_tags(x, 7))\n",
        "\n",
        "# Map names to their top 7 tags\n",
        "name_to_top_tags = pd.Series(df['Top_7_Tags'].values, index=df['Name']).to_dict()\n",
        "\n",
        "# Initialize defaultdict to store counts and names for each tag\n",
        "tag_to_overall_count_and_names = defaultdict(lambda: {\"count\": 0, \"names\": defaultdict(int)})\n",
        "\n",
        "# Update counts and names for each tag\n",
        "for name, tags in name_to_top_tags.items():\n",
        "    for tag in tags:\n",
        "        # Increase the overall count for the tag\n",
        "        tag_to_overall_count_and_names[tag][\"count\"] += 1\n",
        "        # Increase the count for this tag under this specific name\n",
        "        tag_to_overall_count_and_names[tag][\"names\"][name] += 1\n",
        "\n",
        "# Add tag counts and names to DataFrame\n",
        "df['Tag_Counts'] = df['Top_7_Tags'].apply(lambda tags: {tag: tag_to_overall_count_and_names[tag][\"count\"] for tag in tags})\n",
        "df['Tag_Names'] = df['Top_7_Tags'].apply(lambda tags: {tag: \", \".join([f\"{name} - {count}\" for name, count in tag_to_overall_count_and_names[tag][\"names\"].items()]) for tag in tags})\n",
        "\n",
        "for tag, info in tag_to_overall_count_and_names.items():\n",
        "    names_counts = ', '.join([f\"{name} - {count}\" for name, count in info[\"names\"].items()])\n",
        "    print(f\"{tag.capitalize()} - {names_counts}; Total - {info['count']}\")\n",
        "\n",
        "# Save DataFrame to a new CSV file\n",
        "new_file_path = \"/content/New_Final_pd_with_tags.csv\"\n",
        "df.to_csv(new_file_path, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azKG74LSbtp8",
        "outputId": "2d9adac0-c4b7-4ebc-cb89-59764b695c8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarah - Sarah - 1; Total - 1\n",
            "Dedicated - Sarah - 1, Sophia - 1, Lily - 1; Total - 3\n",
            "Student - Sarah - 1, Mia - 1, Sophia - 1, Lily - 1, Max - 1; Total - 5\n",
            "Passion - Sarah - 1, Grace - 1, Charlotte - 1; Total - 3\n",
            "Mathematics - Sarah - 1; Total - 1\n",
            "Physics - Sarah - 1, Leo - 1; Total - 2\n",
            "Interest - Sarah - 1, James - 1, Max - 1; Total - 3\n",
            "Science - Alex - 1, Mia - 1, Olivia - 1, Lily - 1, Zoe - 1, Oliver - 1, Isabella - 1, Gia - 1; Total - 8\n",
            "Alex - Alex - 1; Total - 1\n",
            "Tech - Alex - 1, Oliver - 1; Total - 2\n",
            "Enthusiast - Alex - 1, James - 1, Lucas - 1, Oliver - 1, William - 1; Total - 5\n",
            "Study - Alex - 1, Lucas - 1, Ava - 1, Leo - 1, Amelia - 1, Oliver - 1; Total - 6\n",
            "Computer - Alex - 1, Max - 1, Oliver - 1; Total - 3\n",
            "Data - Alex - 1; Total - 1\n",
            "Creative - Emily - 1; Total - 1\n",
            "Emily - Emily - 1; Total - 1\n",
            "Avid - Emily - 1; Total - 1\n",
            "Reader - Emily - 1; Total - 1\n",
            "Historian - Emily - 1; Total - 1\n",
            "Focus - Emily - 1, Sophia - 1, Lily - 1, Noah - 1, Grace - 1, Henry - 1; Total - 6\n",
            "Literature - Emily - 1, Charlotte - 1; Total - 2\n",
            "James - James - 1; Total - 1\n",
            "Science - James - 1; Total - 1\n",
            "Excel - James - 1; Total - 1\n",
            "Biology - James - 1, Ethan  - 1, Isabella - 1; Total - 3\n",
            "Chemistry - James - 1, Lily - 1; Total - 2\n",
            "Mia - Mia - 1; Total - 1\n",
            "Economics - Mia - 1, Noah - 1, Harper - 1; Total - 3\n",
            "Political - Mia - 1, Zoe - 1, Gia - 1; Total - 3\n",
            "Passionate - Mia - 1; Total - 1\n",
            "Understand - Mia - 1; Total - 1\n",
            "Ryan - Ryan - 1; Total - 1\n",
            "Aspire - Ryan - 1, Max - 1, Oliver - 1; Total - 3\n",
            "Engineer - Ryan - 1; Total - 1\n",
            "Fascinated - Ryan - 1; Total - 1\n",
            "Robotic - Ryan - 1; Total - 1\n",
            "Renewable - Ryan - 1; Total - 1\n",
            "Energy - Ryan - 1; Total - 1\n",
            "Sophia - Sophia - 1; Total - 1\n",
            "Sociology - Sophia - 1; Total - 1\n",
            "Gender - Sophia - 1; Total - 1\n",
            "Studies - Sophia - 1; Total - 1\n",
            "Art - Lucas - 1, Amelia - 1; Total - 2\n",
            "Art - Lucas - 1, Amelia - 1; Total - 2\n",
            "Lucas - Lucas - 1; Total - 1\n",
            "Fine - Lucas - 1; Total - 1\n",
            "Arts - Lucas - 1; Total - 1\n",
            "Genetic - Ethan  - 1; Total - 1\n",
            "Ethan - Ethan  - 1; Total - 1\n",
            "'s - Ethan  - 1, Noah - 1, Mason - 1, Benjamin - 1, Henry - 1; Total - 5\n",
            "Academic - Ethan  - 1, Noah - 1, Benjamin - 1, Henry - 1; Total - 4\n",
            "Journey - Ethan  - 1; Total - 1\n",
            "Genetics - Ethan  - 1; Total - 1\n",
            "Olivia - Olivia - 1; Total - 1\n",
            "Bud - Olivia - 1; Total - 1\n",
            "Environmental - Olivia - 1; Total - 1\n",
            "Scientist - Olivia - 1; Total - 1\n",
            "Specialize - Olivia - 1; Total - 1\n",
            "Environmental - Olivia - 1, Lily - 1, Isabella - 1; Total - 3\n",
            "Lily - Lily - 1; Total - 1\n",
            "Max - Max - 1; Total - 1\n",
            "Engineering - Max - 1, Mason - 1, Benjamin - 1; Total - 3\n",
            "Keen - Max - 1; Total - 1\n",
            "Ava - Ava - 1; Total - 1\n",
            "Delf - Ava - 1, Harper - 1, Amelia - 1, Isabella - 1, Gia - 1; Total - 5\n",
            "World - Ava - 1, Amelia - 1, Isabella - 1, Charlotte - 1; Total - 4\n",
            "Language - Ava - 1; Total - 1\n",
            "Culture - Ava - 1; Total - 1\n",
            "Linguistics - Ava - 1; Total - 1\n",
            "Financial - Noah - 1, Henry - 1; Total - 2\n",
            "Noah - Noah - 1; Total - 1\n",
            "Finance - Noah - 1, Henry - 1; Total - 2\n",
            "Zoe - Zoe - 1; Total - 1\n",
            "Immerse - Zoe - 1, Charlotte - 1; Total - 2\n",
            "International - Zoe - 1; Total - 1\n",
            "Relations - Zoe - 1; Total - 1\n",
            "Drive - Zoe - 1; Total - 1\n",
            "Leo - Leo - 1; Total - 1\n",
            "Explore - Leo - 1, William - 1; Total - 2\n",
            "Wonder - Leo - 1; Total - 1\n",
            "Universe - Leo - 1; Total - 1\n",
            "Astronomy - Leo - 1; Total - 1\n",
            "Psychology - Harper - 1; Total - 1\n",
            "Behavioral - Harper - 1; Total - 1\n",
            "Make - Harper - 1; Total - 1\n",
            "Harper - Harper - 1; Total - 1\n",
            "Realm - Harper - 1, William - 1; Total - 2\n",
            "Robotics - Mason - 1; Total - 1\n",
            "Mason - Mason - 1; Total - 1\n",
            "Pursuit - Mason - 1, Benjamin - 1; Total - 2\n",
            "Mechanical - Mason - 1, Benjamin - 1; Total - 2\n",
            "Reflect - Mason - 1; Total - 1\n",
            "Amelia - Amelia - 1; Total - 1\n",
            "History - Amelia - 1, William - 1; Total - 2\n",
            "Oliver - Oliver - 1; Total - 1\n",
            "Marine - Isabella - 1; Total - 1\n",
            "Isabella - Isabella - 1; Total - 1\n",
            "William - William - 1; Total - 1\n",
            "History - William - 1; Total - 1\n",
            "Archaeology - William - 1; Total - 1\n",
            "Health - Grace - 1; Total - 1\n",
            "Grace - Grace - 1; Total - 1\n",
            "Public - Grace - 1; Total - 1\n",
            "Epidemiology - Grace - 1; Total - 1\n",
            "Disease - Grace - 1; Total - 1\n",
            "Benjamin - Benjamin - 1; Total - 1\n",
            "Aerospace - Benjamin - 1; Total - 1\n",
            "Advocate - Gia - 1; Total - 1\n",
            "Gia - Gia - 1; Total - 1\n",
            "Human - Gia - 1; Total - 1\n",
            "Rights - Gia - 1; Total - 1\n",
            "Henry - Henry - 1; Total - 1\n",
            "Investment - Henry - 1; Total - 1\n",
            "Charlotte - Charlotte - 1; Total - 1\n",
            "Creative - Charlotte - 1; Total - 1\n",
            "Writing - Charlotte - 1; Total - 1\n",
            "Psychology - ; Total - 0\n",
            "Intricacy - ; Total - 0\n",
            "Human - ; Total - 0\n",
            "Behavior - ; Total - 0\n",
            "Aficionado - ; Total - 0\n",
            "Linguistic - ; Total - 0\n",
            "Anthropology - ; Total - 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3xiHAUccCRU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}