{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5qJ91616LyY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import rdflib\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize NLTK tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load RDF data from XML file\n",
        "g = rdflib.Graph()\n",
        "g.parse(\"/content/gauri1.rdf\", format=\"xml\")\n",
        "\n",
        "# Adjusting the SPARQL query\n",
        "query = \"\"\"\n",
        "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "PREFIX school: <http://www.semanticweb.org/guptgau03/ontologies/2024/2/school/>\n",
        "\n",
        "SELECT ?name ?description\n",
        "WHERE {\n",
        "    ?s rdf:type ?type .\n",
        "    ?s school:name ?name .\n",
        "    OPTIONAL { ?s school:description ?description . }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "results = g.query(query)\n",
        "\n",
        "# Prepare the data\n",
        "data = [{'Name': str(row.name), 'Description': str(row.description) if row.description else ''} for row in results]\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame to verify the data is loaded correctly\n",
        "print(df.head(10))\n",
        "\n",
        "def preprocess_and_tokenize(text):\n",
        "    # Convert to lowercase, remove punctuation, tokenize, remove stop words, and lemmatize\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing and other processing steps as before\n",
        "df['Processed'] = df['Description'].apply(preprocess_and_tokenize)\n",
        "\n",
        "def score_terms(tokens):\n",
        "    # Count term frequencies\n",
        "    term_freq = Counter(tokens)\n",
        "    # Sort terms by frequency\n",
        "    sorted_terms = dict(sorted(term_freq.items(), key=lambda item: item[1], reverse=True))\n",
        "    return sorted_terms\n",
        "\n",
        "# Score and rank terms in descriptions\n",
        "df['Ranked_Terms'] = df['Processed'].apply(score_terms)\n",
        "\n",
        "def select_top_n_tags(ranked_terms, n=7):\n",
        "    # Select the top N terms\n",
        "    return list(ranked_terms.keys())[:n]\n",
        "\n",
        "# Select top 7 tags for each description\n",
        "df['Top_7_Tags'] = df['Ranked_Terms'].apply(lambda x: select_top_n_tags(x, 7))\n",
        "\n",
        "# Map names to their top 7 tags\n",
        "name_to_top_tags = pd.Series(df['Top_7_Tags'].values, index=df['Name']).to_dict()\n",
        "\n",
        "# Initialize defaultdict to store counts and names for each tag\n",
        "tag_to_overall_count_and_names = defaultdict(lambda: {\"count\": 0, \"names\": defaultdict(int)})\n",
        "\n",
        "# Update counts and names for each tag\n",
        "for name, tags in name_to_top_tags.items():\n",
        "    for tag in tags:\n",
        "        # Increase the overall count for the tag\n",
        "        tag_to_overall_count_and_names[tag][\"count\"] += 1\n",
        "        # Increase the count for this tag under this specific name\n",
        "        tag_to_overall_count_and_names[tag][\"names\"][name] += 1\n",
        "\n",
        "# Print results\n",
        "for tag, info in tag_to_overall_count_and_names.items():\n",
        "    names_counts = ', '.join([f\"{name} - {count}\" for name, count in info[\"names\"].items()])\n",
        "    print(f\"{tag.capitalize()} - {names_counts}; Total - {info['count']}\")\n"
      ]
    }
  ]
}